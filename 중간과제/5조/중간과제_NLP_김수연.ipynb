{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Assignment: Spam Filter\n",
    "## Import necessary libs and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:38.019436Z",
     "start_time": "2020-10-28T07:39:36.090130Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, autograd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
    "data = pd.read_csv('spam.csv', encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:38.046941Z",
     "start_time": "2020-10-28T07:39:38.022412Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "3   ham  U dun say so early hor... U c already then say...        NaN   \n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  \n",
       "3        NaN        NaN  \n",
       "4        NaN        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:38.086530Z",
     "start_time": "2020-10-28T07:39:38.055901Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (5572, 2)\n",
      "0    4825\n",
      "1     747\n",
      "Name: isSpam, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isSpam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  isSpam\n",
       "0  Go until jurong point, crazy.. Available only ...       0\n",
       "1                      Ok lar... Joking wif u oni...       0\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
       "3  U dun say so early hor... U c already then say...       0\n",
       "4  Nah I don't think he goes to usf, he lives aro...       0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del data['Unnamed: 2']\n",
    "del data['Unnamed: 3']\n",
    "del data['Unnamed: 4']\n",
    "\n",
    "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
    "data['text'] = data['v2']\n",
    "data['isSpam'] = data['v1']\n",
    "\n",
    "del data['v1'], data['v2']\n",
    "\n",
    "print(f'Data Shape: {data.shape}')\n",
    "# imbalanced data\n",
    "print(data['isSpam'].value_counts())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, test split\n",
    "### 평가에 사용할 예정이니 트레인, 테스트 스플릿 코드는 그대로 유지시켜주세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:39.197605Z",
     "start_time": "2020-10-28T07:39:38.101017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5014 558\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = data['text'], data['isSpam']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,\n",
    "                                                   stratify=y, test_size=0.1)\n",
    "\n",
    "print(len(X_train), len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "### 텍스트 전처리함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:39.560602Z",
     "start_time": "2020-10-28T07:39:39.209026Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download()\n",
    "\n",
    "def preprocess(string: str, *args, **kwargs) -> str:\n",
    "    '''\n",
    "    1. remove puctuations\n",
    "    2. remove stop words\n",
    "    3. lower case\n",
    "    '''\n",
    "    # to lower case\n",
    "    low_str = string.lower()\n",
    "    \n",
    "    # remove punctuation\n",
    "    nopunc = re.sub(r'[^\\w\\s]', '', low_str)\n",
    "    \n",
    "    # remove stop words\n",
    "    STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    return ' '.join([word for word in nopunc.split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:40.404808Z",
     "start_time": "2020-10-28T07:39:39.563095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5448                              aight pick open tonight\n",
       "1707                test earlier appreciate call tomorrow\n",
       "2117      wish many many returns day happy birthday vikky\n",
       "1357    good afternoon loverboy goes day luck come way...\n",
       "787     ever thought living good life perfect partner ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Walkthrough\n",
    "cleaned = X_train.apply(preprocess)\n",
    "cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 앞에서 보셨다시피 raw text를 그대로 사용하기엔 무리가 있습니다.(특수기호 및 불용어 문제 등)\n",
    "#### 따라서 전처리되지 않은 raw string을 전처리하는 함수를 만들어주세요. <br>\n",
    "```python\n",
    "preprocess('Helllllo World-!') = 'hello world'\n",
    "```\n",
    "<br>\n",
    "\n",
    "#### ```re``` library를 이용해서 전처리를 쉽게 할 수 있습니다.\n",
    "\n",
    "\n",
    "[re documentation](https://docs.python.org/3/library/re.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "### 전처리된 텍스트를 토크나이징 해주는 함수입니다.\n",
    "#### ```SpaCy, nltk``` 등 영어 tokenizing 라이브러리를 쓰셔도 괜찮습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:40.412788Z",
     "start_time": "2020-10-28T07:39:40.407681Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(string: str, *args, **kwargs) -> list:\n",
    "    '''\n",
    "    return tokenized text as a list\n",
    "    '''\n",
    "    return string.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:40.517904Z",
     "start_time": "2020-10-28T07:39:40.418282Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5448                         [aight, pick, open, tonight]\n",
       "1707          [test, earlier, appreciate, call, tomorrow]\n",
       "2117    [wish, many, many, returns, day, happy, birthd...\n",
       "1357    [good, afternoon, loverboy, goes, day, luck, c...\n",
       "787     [ever, thought, living, good, life, perfect, p...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Walkthrough\n",
    "tokens = cleaned.apply(tokenize)\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Ex) \n",
    "```python\n",
    "tokenize('hello world!',  *args, **kwargs) = ['hello', 'world']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Vocabulary\n",
    "### 토큰들을 이용해서 자주 등장한 순서대로 n개의 원소를 갖는 딕셔너리를 만들어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:40.532477Z",
     "start_time": "2020-10-28T07:39:40.523374Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def build_vocab(n, tokens, *args, **kwargs):\n",
    "    '''\n",
    "    In custom dataset, get list of tokenized sentences as input\n",
    "    padding_idx = 0\n",
    "    unk_idx = 1\n",
    "    '''\n",
    "    # clean_txt = data.text.apply(preprocess)\n",
    "    # tokens = clean_txt.apply(tokenize)\n",
    "    vocab = Counter()\n",
    "\n",
    "    for msg in tokens:\n",
    "        vocab.update(msg)\n",
    "    \n",
    "    vocabulary = dict()\n",
    "    vocabulary['padding_idx'] = 0\n",
    "    vocabulary['unk_idx'] = 1\n",
    "    \n",
    "    for i in range(n-2):\n",
    "        vocabulary[vocab.most_common()[i][0]] = i+2\n",
    "    return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:49.240080Z",
     "start_time": "2020-10-28T07:39:40.536419Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'padding_idx': 0,\n",
       " 'unk_idx': 1,\n",
       " 'call': 2,\n",
       " 'get': 3,\n",
       " 'go': 4,\n",
       " 'free': 5,\n",
       " 'ok': 6,\n",
       " 'ltgt': 7,\n",
       " 'know': 8,\n",
       " 'got': 9,\n",
       " 'like': 10,\n",
       " 'ill': 11,\n",
       " 'good': 12,\n",
       " 'come': 13,\n",
       " 'time': 14,\n",
       " 'day': 15,\n",
       " 'love': 16,\n",
       " 'send': 17,\n",
       " 'want': 18,\n",
       " 'text': 19,\n",
       " 'one': 20,\n",
       " 'going': 21,\n",
       " 'need': 22,\n",
       " 'txt': 23,\n",
       " 'r': 24,\n",
       " 'home': 25,\n",
       " 'lor': 26,\n",
       " 'still': 27,\n",
       " 'see': 28,\n",
       " 'back': 29,\n",
       " 'sorry': 30,\n",
       " 'today': 31,\n",
       " 'stop': 32,\n",
       " 'tell': 33,\n",
       " 'n': 34,\n",
       " 'mobile': 35,\n",
       " 'reply': 36,\n",
       " 'later': 37,\n",
       " 'new': 38,\n",
       " 'hi': 39,\n",
       " 'think': 40,\n",
       " 'well': 41,\n",
       " 'please': 42,\n",
       " 'da': 43,\n",
       " 'cant': 44,\n",
       " 'phone': 45,\n",
       " 'take': 46,\n",
       " 'week': 47,\n",
       " 'night': 48,\n",
       " 'oh': 49,\n",
       " 'ì_': 50,\n",
       " 'hey': 51,\n",
       " 'happy': 52,\n",
       " 'great': 53,\n",
       " 'much': 54,\n",
       " 'dear': 55,\n",
       " 'pls': 56,\n",
       " 'claim': 57,\n",
       " 'hope': 58,\n",
       " 'way': 59,\n",
       " 'make': 60,\n",
       " 'work': 61,\n",
       " 'give': 62,\n",
       " 'thats': 63,\n",
       " 'wat': 64,\n",
       " 'right': 65,\n",
       " 'already': 66,\n",
       " 'say': 67,\n",
       " 'prize': 68,\n",
       " 'ask': 69,\n",
       " 'said': 70,\n",
       " 'number': 71,\n",
       " 'yes': 72,\n",
       " '1': 73,\n",
       " 'yeah': 74,\n",
       " 'c': 75,\n",
       " 'message': 76,\n",
       " 'tomorrow': 77,\n",
       " 'didnt': 78,\n",
       " 'e': 79,\n",
       " 'msg': 80,\n",
       " 'really': 81,\n",
       " 'cos': 82,\n",
       " 'babe': 83,\n",
       " 'meet': 84,\n",
       " 'would': 85,\n",
       " 'amp': 86,\n",
       " 'win': 87,\n",
       " 'every': 88,\n",
       " 'last': 89,\n",
       " 'life': 90,\n",
       " 'miss': 91,\n",
       " 'pick': 92,\n",
       " 'lol': 93,\n",
       " 'find': 94,\n",
       " 'ive': 95,\n",
       " 'cash': 96,\n",
       " 'something': 97,\n",
       " 'morning': 98,\n",
       " 'thanks': 99,\n",
       " 'k': 100,\n",
       " 'let': 101,\n",
       " 'also': 102,\n",
       " 'keep': 103,\n",
       " '3': 104,\n",
       " 'nokia': 105,\n",
       " 'sent': 106,\n",
       " 'anything': 107,\n",
       " 'sure': 108,\n",
       " 'contact': 109,\n",
       " 'went': 110,\n",
       " 'care': 111,\n",
       " 'buy': 112,\n",
       " 'us': 113,\n",
       " 'thing': 114,\n",
       " 'even': 115,\n",
       " 'first': 116,\n",
       " 'chat': 117,\n",
       " 'soon': 118,\n",
       " 'feel': 119,\n",
       " 'around': 120,\n",
       " 'could': 121,\n",
       " 'money': 122,\n",
       " 'urgent': 123,\n",
       " 'tonight': 124,\n",
       " 'gonna': 125,\n",
       " 'b': 126,\n",
       " 'help': 127,\n",
       " 'someone': 128,\n",
       " 'always': 129,\n",
       " 'next': 130,\n",
       " 'per': 131,\n",
       " 'nice': 132,\n",
       " 'service': 133,\n",
       " 'many': 134,\n",
       " 'dun': 135,\n",
       " 'friends': 136,\n",
       " 'place': 137,\n",
       " 'sleep': 138,\n",
       " 'wait': 139,\n",
       " 'late': 140,\n",
       " 'wan': 141,\n",
       " 'customer': 142,\n",
       " 'gud': 143,\n",
       " 'leave': 144,\n",
       " 'ya': 145,\n",
       " 'things': 146,\n",
       " 'told': 147,\n",
       " 'wish': 148,\n",
       " 'year': 149,\n",
       " 'waiting': 150,\n",
       " 'coming': 151,\n",
       " 'getting': 152,\n",
       " 'ìï': 153,\n",
       " 'wont': 154,\n",
       " 'hello': 155,\n",
       " 'people': 156,\n",
       " 'try': 157,\n",
       " 'yet': 158,\n",
       " 'youre': 159,\n",
       " 'x': 160,\n",
       " 'done': 161,\n",
       " '16': 162,\n",
       " 'smile': 163,\n",
       " 'thk': 164,\n",
       " 'v': 165,\n",
       " 'friend': 166,\n",
       " 'haha': 167,\n",
       " 'may': 168,\n",
       " 'fine': 169,\n",
       " 'live': 170,\n",
       " 'thought': 171,\n",
       " 'name': 172,\n",
       " 'class': 173,\n",
       " 'mins': 174,\n",
       " 'lunch': 175,\n",
       " 'tone': 176,\n",
       " 'hows': 177,\n",
       " 'job': 178,\n",
       " 'meeting': 179,\n",
       " 'havent': 180,\n",
       " '18': 181,\n",
       " 'man': 182,\n",
       " 'special': 183,\n",
       " 'bit': 184,\n",
       " 'mind': 185,\n",
       " 'never': 186,\n",
       " 'heart': 187,\n",
       " 'stuff': 188,\n",
       " 'use': 189,\n",
       " 'better': 190,\n",
       " '5': 191,\n",
       " 'min': 192,\n",
       " 'talk': 193,\n",
       " 'yup': 194,\n",
       " 'guaranteed': 195,\n",
       " 'finish': 196,\n",
       " 'sms': 197,\n",
       " 'best': 198,\n",
       " 'long': 199,\n",
       " 'holiday': 200,\n",
       " 'ready': 201,\n",
       " 'eat': 202,\n",
       " 'end': 203,\n",
       " 'check': 204,\n",
       " 'lar': 205,\n",
       " 'trying': 206,\n",
       " 'guess': 207,\n",
       " 'guys': 208,\n",
       " 'latest': 209,\n",
       " 'wanna': 210,\n",
       " 'person': 211,\n",
       " 'chance': 212,\n",
       " 'play': 213,\n",
       " 'car': 214,\n",
       " 'another': 215,\n",
       " 'box': 216,\n",
       " 'dat': 217,\n",
       " 'line': 218,\n",
       " 'birthday': 219,\n",
       " 'yo': 220,\n",
       " 'liao': 221,\n",
       " 'big': 222,\n",
       " 'real': 223,\n",
       " 'quite': 224,\n",
       " 'cool': 225,\n",
       " 'receive': 226,\n",
       " 'days': 227,\n",
       " 'half': 228,\n",
       " 'draw': 229,\n",
       " 'enjoy': 230,\n",
       " 'camera': 231,\n",
       " 'awarded': 232,\n",
       " 'account': 233,\n",
       " 'jus': 234,\n",
       " 'room': 235,\n",
       " 'girl': 236,\n",
       " 'nothing': 237,\n",
       " 'house': 238,\n",
       " 'ever': 239,\n",
       " 'god': 240,\n",
       " 'easy': 241,\n",
       " 'xxx': 242,\n",
       " 'id': 243,\n",
       " 'early': 244,\n",
       " 'po': 245,\n",
       " '1st': 246,\n",
       " 'problem': 247,\n",
       " 'sir': 248,\n",
       " 'luv': 249,\n",
       " 'might': 250,\n",
       " 'den': 251,\n",
       " 'shit': 252,\n",
       " 'landline': 253,\n",
       " 'minutes': 254,\n",
       " 'bad': 255,\n",
       " 'plan': 256,\n",
       " 'forgot': 257,\n",
       " 'actually': 258,\n",
       " 'sweet': 259,\n",
       " 'dinner': 260,\n",
       " 'maybe': 261,\n",
       " 'bed': 262,\n",
       " 'aight': 263,\n",
       " 'word': 264,\n",
       " 'cost': 265,\n",
       " 'watching': 266,\n",
       " '2nd': 267,\n",
       " 'kiss': 268,\n",
       " 'part': 269,\n",
       " 'shows': 270,\n",
       " 'little': 271,\n",
       " 'look': 272,\n",
       " 'probably': 273,\n",
       " 'whats': 274,\n",
       " '150ppm': 275,\n",
       " 'hear': 276,\n",
       " 'pa': 277,\n",
       " '6': 278,\n",
       " 'å1000': 279,\n",
       " 'start': 280,\n",
       " 'face': 281,\n",
       " 'dunno': 282,\n",
       " 'thanx': 283,\n",
       " 'orange': 284,\n",
       " 'video': 285,\n",
       " 'code': 286,\n",
       " 'å2000': 287,\n",
       " 'left': 288,\n",
       " 'shes': 289,\n",
       " 'speak': 290,\n",
       " 'weekend': 291,\n",
       " 'working': 292,\n",
       " 'put': 293,\n",
       " 'world': 294,\n",
       " 'anyway': 295,\n",
       " 'lot': 296,\n",
       " 'bus': 297,\n",
       " 'month': 298,\n",
       " 'hes': 299,\n",
       " 'watch': 300,\n",
       " 'everything': 301,\n",
       " 'baby': 302,\n",
       " 'shall': 303,\n",
       " 'fun': 304,\n",
       " 'princess': 305,\n",
       " 'called': 306,\n",
       " 'thank': 307,\n",
       " 'network': 308,\n",
       " 'selected': 309,\n",
       " 'says': 310,\n",
       " 'sexy': 311,\n",
       " 'wanted': 312,\n",
       " 'tv': 313,\n",
       " 'theres': 314,\n",
       " 'came': 315,\n",
       " 'leh': 316,\n",
       " 'offer': 317,\n",
       " 'pay': 318,\n",
       " 'bring': 319,\n",
       " 'looking': 320,\n",
       " 'shopping': 321,\n",
       " 'evening': 322,\n",
       " 'reach': 323,\n",
       " 'ah': 324,\n",
       " 'goes': 325,\n",
       " 'join': 326,\n",
       " 'years': 327,\n",
       " 'remember': 328,\n",
       " '7': 329,\n",
       " 'two': 330,\n",
       " 'tcs': 331,\n",
       " 'entry': 332,\n",
       " 'juz': 333,\n",
       " 'award': 334,\n",
       " 'boy': 335,\n",
       " 'xmas': 336,\n",
       " 'apply': 337,\n",
       " 'å150': 338,\n",
       " 'afternoon': 339,\n",
       " 'fuck': 340,\n",
       " 'important': 341,\n",
       " 'ringtone': 342,\n",
       " 'without': 343,\n",
       " 'mail': 344,\n",
       " 'texts': 345,\n",
       " 'enough': 346,\n",
       " 'school': 347,\n",
       " 'details': 348,\n",
       " 'wake': 349,\n",
       " 'hour': 350,\n",
       " 'sat': 351,\n",
       " 'collection': 352,\n",
       " 'dad': 353,\n",
       " 'since': 354,\n",
       " 'doesnt': 355,\n",
       " 'hav': 356,\n",
       " 'collect': 357,\n",
       " 'guy': 358,\n",
       " 'abt': 359,\n",
       " 'able': 360,\n",
       " 'todays': 361,\n",
       " 'wif': 362,\n",
       " 'stay': 363,\n",
       " 'town': 364,\n",
       " 'times': 365,\n",
       " 'tones': 366,\n",
       " 'must': 367,\n",
       " 'means': 368,\n",
       " 'attempt': 369,\n",
       " 'plz': 370,\n",
       " 'wot': 371,\n",
       " 'though': 372,\n",
       " 'wen': 373,\n",
       " 'wk': 374,\n",
       " 'pain': 375,\n",
       " 'asked': 376,\n",
       " 'away': 377,\n",
       " 'bt': 378,\n",
       " 'haf': 379,\n",
       " 'making': 380,\n",
       " 'double': 381,\n",
       " 'okay': 382,\n",
       " 'made': 383,\n",
       " 'hot': 384,\n",
       " '500': 385,\n",
       " '150p': 386,\n",
       " 'tmr': 387,\n",
       " 'else': 388,\n",
       " 'mob': 389,\n",
       " 'update': 390,\n",
       " 'office': 391,\n",
       " 'driving': 392,\n",
       " 'valid': 393,\n",
       " 'goin': 394,\n",
       " 'oso': 395,\n",
       " 'book': 396,\n",
       " 'bored': 397,\n",
       " 'til': 398,\n",
       " 'dude': 399,\n",
       " 'show': 400,\n",
       " 'run': 401,\n",
       " 'tot': 402,\n",
       " 'rate': 403,\n",
       " 'calls': 404,\n",
       " 'hurt': 405,\n",
       " 'music': 406,\n",
       " 'till': 407,\n",
       " 'bonus': 408,\n",
       " 'true': 409,\n",
       " '9': 410,\n",
       " 'worry': 411,\n",
       " 'full': 412,\n",
       " 'dis': 413,\n",
       " 'missing': 414,\n",
       " 'answer': 415,\n",
       " 'nite': 416,\n",
       " 'colour': 417,\n",
       " 'food': 418,\n",
       " 'alright': 419,\n",
       " 'club': 420,\n",
       " 'plus': 421,\n",
       " 'sad': 422,\n",
       " 'believe': 423,\n",
       " '8007': 424,\n",
       " 'smiling': 425,\n",
       " 'wid': 426,\n",
       " 'ard': 427,\n",
       " 'hair': 428,\n",
       " 'words': 429,\n",
       " 'lei': 430,\n",
       " 'weekly': 431,\n",
       " 'missed': 432,\n",
       " 'yesterday': 433,\n",
       " 'price': 434,\n",
       " 'saying': 435,\n",
       " 'saw': 436,\n",
       " 'lose': 437,\n",
       " 'test': 438,\n",
       " 'online': 439,\n",
       " 'trip': 440,\n",
       " 'leaving': 441,\n",
       " 'hours': 442,\n",
       " 'å100': 443,\n",
       " 'gift': 444,\n",
       " 'together': 445,\n",
       " 'tried': 446,\n",
       " 'minute': 447,\n",
       " 'change': 448,\n",
       " 'g': 449,\n",
       " 'order': 450,\n",
       " 'ltdecimalgt': 451,\n",
       " 'points': 452,\n",
       " 'thinking': 453,\n",
       " 'makes': 454,\n",
       " 'wife': 455,\n",
       " 'mean': 456,\n",
       " 'tc': 457,\n",
       " 'lets': 458,\n",
       " 'calling': 459,\n",
       " 'story': 460,\n",
       " '86688': 461,\n",
       " 'wants': 462,\n",
       " 'top': 463,\n",
       " 'address': 464,\n",
       " 'either': 465,\n",
       " 'set': 466,\n",
       " 'gr8': 467,\n",
       " 'charge': 468,\n",
       " 'sch': 469,\n",
       " 'congrats': 470,\n",
       " 'aft': 471,\n",
       " 'question': 472,\n",
       " 'email': 473,\n",
       " 'pics': 474,\n",
       " 'private': 475,\n",
       " 'de': 476,\n",
       " 'drive': 477,\n",
       " 'friendship': 478,\n",
       " 'feeling': 479,\n",
       " 'sae': 480,\n",
       " 'noe': 481,\n",
       " 'date': 482,\n",
       " 'everyone': 483,\n",
       " 'happen': 484,\n",
       " 'game': 485,\n",
       " 'head': 486,\n",
       " 'old': 487,\n",
       " 'cause': 488,\n",
       " 'card': 489,\n",
       " 'taking': 490,\n",
       " 'second': 491,\n",
       " 'okie': 492,\n",
       " 'busy': 493,\n",
       " 'delivery': 494,\n",
       " 'close': 495,\n",
       " 'took': 496,\n",
       " 'lucky': 497,\n",
       " 'tomo': 498,\n",
       " 'poly': 499,\n",
       " 'huh': 500,\n",
       " 'anyone': 501,\n",
       " 'å5000': 502,\n",
       " 'auction': 503,\n",
       " 'await': 504,\n",
       " 'brother': 505,\n",
       " 'messages': 506,\n",
       " 'pounds': 507,\n",
       " 'choose': 508,\n",
       " 'weeks': 509,\n",
       " 'eve': 510,\n",
       " 'whatever': 511,\n",
       " 'coz': 512,\n",
       " 'simple': 513,\n",
       " 'search': 514,\n",
       " 'family': 515,\n",
       " 'neva': 516,\n",
       " 'girls': 517,\n",
       " 'found': 518,\n",
       " 'gd': 519,\n",
       " 'voucher': 520,\n",
       " 'lesson': 521,\n",
       " 'rite': 522,\n",
       " 'angry': 523,\n",
       " 'isnt': 524,\n",
       " 'uk': 525,\n",
       " '750': 526,\n",
       " 'statement': 527,\n",
       " 'expires': 528,\n",
       " 'mobileupd8': 529,\n",
       " 'company': 530,\n",
       " '12hrs': 531,\n",
       " 'beautiful': 532,\n",
       " 'pub': 533,\n",
       " 'unsubscribe': 534,\n",
       " 'prob': 535,\n",
       " 'final': 536,\n",
       " 'dating': 537,\n",
       " 'vouchers': 538,\n",
       " 'open': 539,\n",
       " 'sea': 540,\n",
       " '150pmsg': 541,\n",
       " 'alone': 542,\n",
       " 'visit': 543,\n",
       " 'happened': 544,\n",
       " 'å250': 545,\n",
       " 'tscs': 546,\n",
       " 'sis': 547,\n",
       " 'nope': 548,\n",
       " '100': 549,\n",
       " 'fucking': 550,\n",
       " 'ltd': 551,\n",
       " 'unredeemed': 552,\n",
       " 'identifier': 553,\n",
       " '08000839402': 554,\n",
       " 'fast': 555,\n",
       " 'drink': 556,\n",
       " 'comes': 557,\n",
       " 'frnd': 558,\n",
       " 'sleeping': 559,\n",
       " 'services': 560,\n",
       " 'break': 561,\n",
       " 'drop': 562,\n",
       " 'saturday': 563,\n",
       " 'pobox': 564,\n",
       " 'mine': 565,\n",
       " 'secret': 566,\n",
       " 'boytoy': 567,\n",
       " 'kind': 568,\n",
       " 'ring': 569,\n",
       " 'bday': 570,\n",
       " 'å500': 571,\n",
       " 'far': 572,\n",
       " 'shop': 573,\n",
       " 'treat': 574,\n",
       " 'dreams': 575,\n",
       " 'mum': 576,\n",
       " 'college': 577,\n",
       " 'loving': 578,\n",
       " 'camcorder': 579,\n",
       " '08000930705': 580,\n",
       " '10': 581,\n",
       " 'bout': 582,\n",
       " 'sun': 583,\n",
       " 'log': 584,\n",
       " 'anytime': 585,\n",
       " 'welcome': 586,\n",
       " 'party': 587,\n",
       " 'listen': 588,\n",
       " 'phones': 589,\n",
       " 'hand': 590,\n",
       " 'nt': 591,\n",
       " 'mate': 592,\n",
       " '12': 593,\n",
       " 'lots': 594,\n",
       " 'forget': 595,\n",
       " 'land': 596,\n",
       " 'smth': 597,\n",
       " 'smoke': 598,\n",
       " 'available': 599,\n",
       " 'mu': 600,\n",
       " 'knw': 601,\n",
       " 'hard': 602,\n",
       " 'finally': 603,\n",
       " 'ha': 604,\n",
       " 'youve': 605,\n",
       " 'carlos': 606,\n",
       " 'started': 607,\n",
       " 'lovely': 608,\n",
       " 'mates': 609,\n",
       " 'games': 610,\n",
       " 'sister': 611,\n",
       " 'pic': 612,\n",
       " 'used': 613,\n",
       " 'thinks': 614,\n",
       " 'walk': 615,\n",
       " 'worth': 616,\n",
       " 'awesome': 617,\n",
       " 'tho': 618,\n",
       " 'ten': 619,\n",
       " 'movie': 620,\n",
       " 'finished': 621,\n",
       " 'cum': 622,\n",
       " 'post': 623,\n",
       " 'sounds': 624,\n",
       " 'case': 625,\n",
       " 'quiz': 626,\n",
       " 'etc': 627,\n",
       " '8': 628,\n",
       " 'winner': 629,\n",
       " 'crazy': 630,\n",
       " 'side': 631,\n",
       " 'ass': 632,\n",
       " 'wil': 633,\n",
       " 'enter': 634,\n",
       " 'takes': 635,\n",
       " 'talking': 636,\n",
       " 'parents': 637,\n",
       " 'sex': 638,\n",
       " 'fri': 639,\n",
       " 'motorola': 640,\n",
       " 'hungry': 641,\n",
       " 'touch': 642,\n",
       " 'mrng': 643,\n",
       " 'youll': 644,\n",
       " 'mayb': 645,\n",
       " 'type': 646,\n",
       " 'reason': 647,\n",
       " 'project': 648,\n",
       " 'credit': 649,\n",
       " 'mah': 650,\n",
       " 'news': 651,\n",
       " 'å350': 652,\n",
       " 'txts': 653,\n",
       " 'chennai': 654,\n",
       " 'invited': 655,\n",
       " 'fancy': 656,\n",
       " 'msgs': 657,\n",
       " 'course': 658,\n",
       " 'yr': 659,\n",
       " 'caller': 660,\n",
       " 'least': 661,\n",
       " 'decided': 662,\n",
       " 'download': 663,\n",
       " 'b4': 664,\n",
       " 'gone': 665,\n",
       " 'pretty': 666,\n",
       " 'content': 667,\n",
       " 'snow': 668,\n",
       " 'pm': 669,\n",
       " 'valued': 670,\n",
       " 'reading': 671,\n",
       " 'hit': 672,\n",
       " '10p': 673,\n",
       " 'oredi': 674,\n",
       " 'surprise': 675,\n",
       " 'darlin': 676,\n",
       " 'months': 677,\n",
       " '0800': 678,\n",
       " 'area': 679,\n",
       " 'seen': 680,\n",
       " 'friday': 681,\n",
       " 'th': 682,\n",
       " 'gas': 683,\n",
       " 'û_': 684,\n",
       " '2003': 685,\n",
       " '800': 686,\n",
       " 'frnds': 687,\n",
       " 'yar': 688,\n",
       " 'eg': 689,\n",
       " 'happiness': 690,\n",
       " 'mom': 691,\n",
       " 'eh': 692,\n",
       " 'dnt': 693,\n",
       " 'mr': 694,\n",
       " 'lost': 695,\n",
       " 'currently': 696,\n",
       " 'park': 697,\n",
       " 'wit': 698,\n",
       " 'wonderful': 699,\n",
       " 'st': 700,\n",
       " 'blue': 701,\n",
       " 'operator': 702,\n",
       " 'telling': 703,\n",
       " 'wasnt': 704,\n",
       " 'seeing': 705,\n",
       " 'needs': 706,\n",
       " 'read': 707,\n",
       " 'light': 708,\n",
       " 'swing': 709,\n",
       " 'semester': 710,\n",
       " 'ni8': 711,\n",
       " 'national': 712,\n",
       " 'wkly': 713,\n",
       " 'bslvyl': 714,\n",
       " 'wrong': 715,\n",
       " 'outside': 716,\n",
       " 'questions': 717,\n",
       " 'song': 718,\n",
       " 'knew': 719,\n",
       " 'almost': 720,\n",
       " 'picking': 721,\n",
       " 'askd': 722,\n",
       " 'computer': 723,\n",
       " 'congratulations': 724,\n",
       " 'opt': 725,\n",
       " 'cut': 726,\n",
       " 'earlier': 727,\n",
       " 'age': 728,\n",
       " 'support': 729,\n",
       " 'entered': 730,\n",
       " 'correct': 731,\n",
       " 'within': 732,\n",
       " 'uncle': 733,\n",
       " 'point': 734,\n",
       " 'information': 735,\n",
       " 'fr': 736,\n",
       " 'unlimited': 737,\n",
       " 'gym': 738,\n",
       " 'player': 739,\n",
       " 'direct': 740,\n",
       " 'mobiles': 741,\n",
       " 'usf': 742,\n",
       " 'fone': 743,\n",
       " 'ago': 744,\n",
       " 'hold': 745,\n",
       " 'dogging': 746,\n",
       " '150': 747,\n",
       " 'laptop': 748,\n",
       " 'wana': 749,\n",
       " 'loved': 750,\n",
       " 'rock': 751,\n",
       " 'wine': 752,\n",
       " 'goodmorning': 753,\n",
       " 'sim': 754,\n",
       " 'offers': 755,\n",
       " 'hell': 756,\n",
       " 'march': 757,\n",
       " 'grins': 758,\n",
       " 'hmm': 759,\n",
       " '87066': 760,\n",
       " 'shower': 761,\n",
       " 'tired': 762,\n",
       " 'confirm': 763,\n",
       " 'slow': 764,\n",
       " 'sort': 765,\n",
       " 'abiola': 766,\n",
       " 'wonder': 767,\n",
       " 'muz': 768,\n",
       " 'w': 769,\n",
       " 'bank': 770,\n",
       " 'difficult': 771,\n",
       " 'felt': 772,\n",
       " 'meant': 773,\n",
       " 'cs': 774,\n",
       " 'tel': 775,\n",
       " 'luck': 776,\n",
       " 'across': 777,\n",
       " 'reward': 778,\n",
       " 'india': 779,\n",
       " 'met': 780,\n",
       " 'bathe': 781,\n",
       " 'laugh': 782,\n",
       " 'bill': 783,\n",
       " 'ans': 784,\n",
       " 'gotta': 785,\n",
       " 'comin': 786,\n",
       " 'checking': 787,\n",
       " 'different': 788,\n",
       " 'freemsg': 789,\n",
       " 'wow': 790,\n",
       " 'link': 791,\n",
       " 'darren': 792,\n",
       " 'fantastic': 793,\n",
       " 'crave': 794,\n",
       " 'hoping': 795,\n",
       " 'small': 796,\n",
       " 'john': 797,\n",
       " 'empty': 798,\n",
       " 'die': 799,\n",
       " 'wishing': 800,\n",
       " 'sending': 801,\n",
       " 'rply': 802,\n",
       " 'whole': 803,\n",
       " 'hee': 804,\n",
       " 'pass': 805,\n",
       " 'ntt': 806,\n",
       " 'ipod': 807,\n",
       " 'supposed': 808,\n",
       " 'em': 809,\n",
       " 'gets': 810,\n",
       " 'immediately': 811,\n",
       " 'xx': 812,\n",
       " 'whos': 813,\n",
       " 'complimentary': 814,\n",
       " 'truth': 815,\n",
       " 'orchard': 816,\n",
       " 'comp': 817,\n",
       " 'slowly': 818,\n",
       " 'asking': 819,\n",
       " 'somebody': 820,\n",
       " 'heard': 821,\n",
       " 'couple': 822,\n",
       " 'chikku': 823,\n",
       " 'moment': 824,\n",
       " 'rental': 825,\n",
       " 'extra': 826,\n",
       " 'charged': 827,\n",
       " 'savamob': 828,\n",
       " 'hospital': 829,\n",
       " 'ldn': 830,\n",
       " 'admirer': 831,\n",
       " 'bcoz': 832,\n",
       " 'kids': 833,\n",
       " 'txting': 834,\n",
       " 'sell': 835,\n",
       " 'frm': 836,\n",
       " 'wap': 837,\n",
       " 'figure': 838,\n",
       " 'ex': 839,\n",
       " 'gal': 840,\n",
       " 'monday': 841,\n",
       " 'uks': 842,\n",
       " 'match': 843,\n",
       " 'cheap': 844,\n",
       " 'mm': 845,\n",
       " 'wats': 846,\n",
       " 'exam': 847,\n",
       " 'asap': 848,\n",
       " '10pmin': 849,\n",
       " 'less': 850,\n",
       " 'drugs': 851,\n",
       " 'nah': 852,\n",
       " 'hmv': 853,\n",
       " 'poor': 854,\n",
       " 'numbers': 855,\n",
       " 'valentines': 856,\n",
       " 'voice': 857,\n",
       " 'normal': 858,\n",
       " 'catch': 859,\n",
       " 'doctor': 860,\n",
       " 'studying': 861,\n",
       " 'worries': 862,\n",
       " 'colleagues': 863,\n",
       " 'info': 864,\n",
       " 'tickets': 865,\n",
       " 'via': 866,\n",
       " 'christmas': 867,\n",
       " 'paper': 868,\n",
       " 'remove': 869,\n",
       " 'rent': 870,\n",
       " 'rs': 871,\n",
       " 'move': 872,\n",
       " 'mode': 873,\n",
       " 'stupid': 874,\n",
       " 'optout': 875,\n",
       " 'ho': 876,\n",
       " 'warm': 877,\n",
       " 'bath': 878,\n",
       " 'rain': 879,\n",
       " 'ugh': 880,\n",
       " 'discount': 881,\n",
       " 'sick': 882,\n",
       " 'pete': 883,\n",
       " 'convey': 884,\n",
       " 'sunshine': 885,\n",
       " 'sony': 886,\n",
       " 'worried': 887,\n",
       " 'plans': 888,\n",
       " 'safe': 889,\n",
       " 'near': 890,\n",
       " 'otherwise': 891,\n",
       " 'decide': 892,\n",
       " 'ones': 893,\n",
       " 'using': 894,\n",
       " 'leaves': 895,\n",
       " 'booked': 896,\n",
       " 'brings': 897,\n",
       " 'kate': 898,\n",
       " 'promise': 899,\n",
       " 'rest': 900,\n",
       " 'ends': 901,\n",
       " 'oops': 902,\n",
       " 'especially': 903,\n",
       " 'aint': 904,\n",
       " 'buying': 905,\n",
       " 'gave': 906,\n",
       " 'na': 907,\n",
       " 'gods': 908,\n",
       " 'kinda': 909,\n",
       " 'terms': 910,\n",
       " 'spend': 911,\n",
       " 'save': 912,\n",
       " 'unless': 913,\n",
       " 'charity': 914,\n",
       " '2nite': 915,\n",
       " 'hmmm': 916,\n",
       " 'del': 917,\n",
       " 'å200': 918,\n",
       " 'loads': 919,\n",
       " 'energy': 920,\n",
       " 'jay': 921,\n",
       " 'accept': 922,\n",
       " 'knows': 923,\n",
       " 'wondering': 924,\n",
       " 'starts': 925,\n",
       " 'sound': 926,\n",
       " 'dream': 927,\n",
       " 'ûò': 928,\n",
       " 'std': 929,\n",
       " 'england': 930,\n",
       " 'flag': 931,\n",
       " 'lover': 932,\n",
       " 'summer': 933,\n",
       " 'moms': 934,\n",
       " 'lazy': 935,\n",
       " 'store': 936,\n",
       " 'yep': 937,\n",
       " 'loan': 938,\n",
       " 'ending': 939,\n",
       " 'rates': 940,\n",
       " 'ice': 941,\n",
       " 'tonite': 942,\n",
       " 'understand': 943,\n",
       " 'sale': 944,\n",
       " 'disturb': 945,\n",
       " 'add': 946,\n",
       " 'style': 947,\n",
       " 'btnationalrate': 948,\n",
       " 'tampa': 949,\n",
       " 'woke': 950,\n",
       " 'sofa': 951,\n",
       " 'user': 952,\n",
       " 'awaiting': 953,\n",
       " '11': 954,\n",
       " 'planning': 955,\n",
       " 'bak': 956,\n",
       " 'feels': 957,\n",
       " 'write': 958,\n",
       " 'wed': 959,\n",
       " 'film': 960,\n",
       " 'glad': 961,\n",
       " 'mood': 962,\n",
       " 'theyre': 963,\n",
       " 'wwwgetzedcouk': 964,\n",
       " 'youd': 965,\n",
       " 'fact': 966,\n",
       " 'wishes': 967,\n",
       " 'dvd': 968,\n",
       " 'train': 969,\n",
       " 'representative': 970,\n",
       " 'nyt': 971,\n",
       " 'study': 972,\n",
       " 'seriously': 973,\n",
       " 'daddy': 974,\n",
       " 'trust': 975,\n",
       " 'hgsuite3422lands': 976,\n",
       " 'urself': 977,\n",
       " 'lessons': 978,\n",
       " 'ave': 979,\n",
       " 'forwarded': 980,\n",
       " 'round': 981,\n",
       " 'weekends': 982,\n",
       " 'facebook': 983,\n",
       " 'silent': 984,\n",
       " 'credits': 985,\n",
       " 'pizza': 986,\n",
       " 'completely': 987,\n",
       " 'lect': 988,\n",
       " 'whenever': 989,\n",
       " 'father': 990,\n",
       " 'ufind': 991,\n",
       " 'rreveal': 992,\n",
       " 'specialcall': 993,\n",
       " 'staying': 994,\n",
       " 'p': 995,\n",
       " 'thinkin': 996,\n",
       " 'hiya': 997,\n",
       " 'cinema': 998,\n",
       " 'nigeria': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Walkthrough\n",
    "vocab = build_vocab(5000, tokens)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Ex) \n",
    "```python\n",
    "vocab = build_vocab(4, *args, **kwargs)\n",
    "vocab = {'padding_idx': 0, 'unk_idx': 1, 'hello': 2, 'world': 3}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 여기서 ```padding_idx```는 패딩에 쓰이는 인덱스, ```unk_idx```는 unknown token을 의미합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toTensor\n",
    "#### 토큰들을 텐서로 바꿔주는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:49.253241Z",
     "start_time": "2020-10-28T07:39:49.242459Z"
    }
   },
   "outputs": [],
   "source": [
    "def toTensor(max_length, tokens, vocab, labels, *args, **kwargs):\n",
    "    # Tokenize & Vectorize sequences\n",
    "    vectorized_seqs = []\n",
    "    for msg in tokens:\n",
    "        vec = [0] * len(msg)\n",
    "        for i, token in enumerate(msg):\n",
    "            if token in vocab.keys():\n",
    "                vec[i] = vocab[token]\n",
    "            else:\n",
    "                vec[i] = vocab['unk_idx']\n",
    "        vectorized_seqs.append(vec)\n",
    "\n",
    "    # 전처리 과정에서 seq_length = 0 되는 것들 제거\n",
    "    # e.g. pd.Series(vec_seq)[pd.Series(vec_seq).apply(sum) == 0]\n",
    "    label_tensor = torch.tensor(np.array(labels))\n",
    "    idx = pd.Series(vectorized_seqs)[pd.Series(vectorized_seqs).apply(sum) != 0].index\n",
    "    vectorized_seqs = pd.Series(vectorized_seqs)[idx]\n",
    "    label_tensor = pd.Series(label_tensor)[idx]\n",
    "    vectorized_seqs = vectorized_seqs.tolist()\n",
    "    label_tensor = torch.tensor(np.array(label_tensor))\n",
    "    \n",
    "    # Save the lengths of sequences\n",
    "    seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
    "    \n",
    "    \n",
    "    # Add padding(0)\n",
    "    seq_tensor = Variable(torch.zeros((len(vectorized_seqs), max_length))).long()\n",
    "    for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
    "        \n",
    "        if seqlen <= max_length:\n",
    "            seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
    "        else:\n",
    "            seq_tensor[idx, ] = torch.LongTensor(seq[:max_length])\n",
    "   \n",
    "    return seq_tensor, seq_lengths, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:49.497133Z",
     "start_time": "2020-10-28T07:39:49.255971Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5006 5006 5006\n"
     ]
    }
   ],
   "source": [
    "# Walkthrough\n",
    "seq_tensor, seq_lengths, label_tensor = toTensor(100, tokens, vocab, y_train)\n",
    "print(len(seq_tensor), len(seq_lengths), len(label_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "시퀀스의 max length가 5일 때 다음과 같습니다.\n",
    "<br>\n",
    "\n",
    "Ex)\n",
    "```python\n",
    "toTensor(5, ['hello', 'world!', 'yonsei']) = torch.LongTensor([2, 3, 1, 0, 0])\n",
    "```\n",
    "\n",
    "여기서 ```yonsei``` 단어는 아까 만든 단어장(vocab)에 포함되지 않은 단어로 ```unk_idx```로 처리됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 위의 함수들을 이용하고 적절한 코드 및 parameter를 적용해서 \n",
    "### MailDataset과 train에 쓸 DataLoader를 만들어주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:39:49.518061Z",
     "start_time": "2020-10-28T07:39:49.500221Z"
    }
   },
   "outputs": [],
   "source": [
    "# train/test, input/label을 앞서 나눠놓았으므로 shuffle은 하지 않는걸로,,, 귀찮,,,\n",
    "import torch.utils.data.sampler as splr\n",
    "\n",
    "class CustomDataLoader(object):\n",
    "    def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_tensor = seq_tensor\n",
    "        self.seq_lengths = seq_lengths\n",
    "        self.label_tensor = label_tensor\n",
    "        self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
    "        self.sampler_iter = iter(self.sampler)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
    "        return self\n",
    "\n",
    "    def _next_index(self):\n",
    "        return next(self.sampler_iter) # may raise StopIteration\n",
    "\n",
    "    def __next__(self):\n",
    "        index = self._next_index()\n",
    "\n",
    "        subset_seq_tensor = self.seq_tensor[index]\n",
    "        subset_seq_lengths = self.seq_lengths[index]\n",
    "        subset_label_tensor = self.label_tensor[index]\n",
    "\n",
    "        # order by length to use pack_padded_sequence()\n",
    "        subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
    "        subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
    "        subset_label_tensor = subset_label_tensor[perm_idx]\n",
    "\n",
    "        return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:40:01.928863Z",
     "start_time": "2020-10-28T07:39:49.521215Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "max_length = 100\n",
    "\n",
    "train_cleaned, test_cleaned = X_train.apply(preprocess), X_test.apply(preprocess)\n",
    "\n",
    "train_tokens, test_tokens = train_cleaned.apply(tokenize), test_cleaned.apply(tokenize)\n",
    "\n",
    "vocab = build_vocab(5000, X.apply(preprocess).apply(tokenize))\n",
    "\n",
    "train_seq_tensor, train_seq_lengths, train_label = toTensor(max_length, train_tokens, vocab, y_train)\n",
    "test_seq_tensor, test_seq_lengths, test_label = toTensor(max_length, test_tokens, vocab, y_test)\n",
    "\n",
    "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
    "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)\n",
    "\n",
    "train_size = len(train_loader.seq_tensor)\n",
    "test_size = len(test_loader.seq_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:40:01.938666Z",
     "start_time": "2020-10-28T07:40:01.931668Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5006 5006 5006\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "print(len(train_loader.seq_tensor), len(train_loader.seq_lengths), len(train_loader.label_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 훈련 인스턴스를 사용해서 train 함수를 통해 training을 해주시고,\n",
    "### eval 함수를 통해 40개의 test example에 대해서 accuracy를 측정해주세요.\n",
    "### 함수 및 클래스 signature와 내부 코드는 적절히 알아서 짜주시면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:40:01.956605Z",
     "start_time": "2020-10-28T07:40:01.943255Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# torch.manual_seed(1)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(1)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:40:01.989688Z",
     "start_time": "2020-10-28T07:40:01.963951Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpamClassifier(nn.Module):\n",
    "    '''\n",
    "    vanila LSTM\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers,\\\n",
    "                 drop_lstm=0.1, drop_out = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_lstm, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "\n",
    "        # embeddings\n",
    "        embedded_seq_tensor = self.embedding(x)\n",
    "                \n",
    "        # pack, remove pads\n",
    "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        \n",
    "        # lstm\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
    "          # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html\n",
    "          # If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero\n",
    "\n",
    "        # unpack, recover padded sequence\n",
    "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "       \n",
    "        # collect the last output in each batch\n",
    "        # last_idxs = (input_sizes - 1).to(device)\n",
    "        last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
    "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output).squeeze()\n",
    "               \n",
    "        # sigmoid function\n",
    "        output = self.sig(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:40:02.019124Z",
     "start_time": "2020-10-28T07:40:01.996004Z"
    }
   },
   "outputs": [],
   "source": [
    "class SpamClassifier2(nn.Module):\n",
    "    '''\n",
    "    Bi-LSTM\n",
    "    '''\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers,\n",
    "                 drop_lstm=0.1, drop_out = 0.1):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_lstm, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(drop_out)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, seq_lengths):\n",
    "\n",
    "        # embeddings\n",
    "        embedded_seq_tensor = self.embedding(x)\n",
    "                \n",
    "        # pack, remove pads\n",
    "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
    "        # [num_layers*2, batch_size, hidden_dim]\n",
    "        (h_0, c_0) = (Variable(torch.zeros(self.n_layers*2, x.size(0), self.hidden_dim)),\n",
    "                      Variable(torch.zeros(self.n_layers*2, x.size(0), self.hidden_dim)))\n",
    "        # lstm\n",
    "        packed_output, (ht, ct) = self.lstm(packed_input, (h_0, c_0))\n",
    "        # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html\n",
    "        # If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero\n",
    "\n",
    "        # unpack, recover padded sequence\n",
    "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "       \n",
    "        # collect the last output in each batch\n",
    "        # last_idxs = (input_sizes - 1).to(device)\n",
    "        last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
    "        # print(last_idxs.size())\n",
    "        # print(last_idxs.view(-1, 1).size())\n",
    "        # print(last_idxs.view(-1, 1).unsqueeze(2).size())\n",
    "        # print(last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim).size())\n",
    "        # print(last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim).squeeze().size())\n",
    "        # output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
    "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, 2*self.hidden_dim)).squeeze()\n",
    "        # print(output.size())\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        output = self.dropout(output)\n",
    "        output = self.fc(output).squeeze()\n",
    "               \n",
    "        # sigmoid function\n",
    "        output = self.sig(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:52:33.122854Z",
     "start_time": "2020-10-28T07:52:33.105942Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, loss_criterion, optimizer, epochs):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    clip = 5 # gradient clipping\n",
    "    history = []\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(\"Epoch: {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        # Set to training mode\n",
    "        model.train()\n",
    "        \n",
    "        # Loss and Accuracy within the epoch\n",
    "        train_loss = 0.0\n",
    "        train_acc = 0.0\n",
    "        i = 0\n",
    "        \n",
    "        for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
    "            i += 1\n",
    "            \n",
    "            seq_tensor = seq_tensor.to(device)\n",
    "            seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
    "            label = label.to(device)\n",
    " \n",
    "            # get the output from the model\n",
    "            output = net(seq_tensor, seq_tensor_lengths)\n",
    "    \n",
    "            # get the loss and backprop\n",
    "            loss = loss_criterion(output, label.float())\n",
    "            optimizer.zero_grad() \n",
    "            loss.backward()\n",
    "        \n",
    "            # prevent the exploding gradient\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute the total loss for the batch and add it to train_loss\n",
    "            train_loss += loss.item() * seq_tensor.size(0)\n",
    "            \n",
    "            # Compute the accuracy\n",
    "            binary_output = (output >= 0.5).short() # short(): torch.int16\n",
    "            correct_counts = torch.eq(binary_output, label)\n",
    "            \n",
    "            # Convert correct_counts to float and then compute the mean\n",
    "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
    "            \n",
    "            # Compute total accuracy in the whole batch and add to train_acc\n",
    "            train_acc += acc.item() * seq_tensor.size(0)\n",
    "            \n",
    "            if i%20==0:\n",
    "                print(\"Batch number: {}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n",
    "        \n",
    "        # Find average training loss and training accuracy\n",
    "        avg_train_loss = train_loss/train_size\n",
    "        avg_train_acc = train_acc/train_size\n",
    "        print(\"Epoch : {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}%\".format(epoch+1, avg_train_loss, avg_train_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:40:02.065359Z",
     "start_time": "2020-10-28T07:40:02.048689Z"
    }
   },
   "outputs": [],
   "source": [
    "def eval(model, criterion):\n",
    "    test_losses = []\n",
    "    sums = []\n",
    "    sizes = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    test_losses = []\n",
    "    for seq_tensor, seq_tensor_lengths, label in iter(test_loader):\n",
    "        seq_tensor = seq_tensor.to(device)\n",
    "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
    "        label = label.to(device)\n",
    "        output = net(seq_tensor, seq_tensor_lengths)\n",
    "        \n",
    "        # losses\n",
    "        test_loss = criterion(output, label.float())     \n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        # accuracy\n",
    "        binary_output = (output >= 0.5).short() # short(): torch.int16\n",
    "        right_or_not = torch.eq(binary_output, label)\n",
    "        sums.append(torch.sum(right_or_not).float().item())\n",
    "        sizes.append(right_or_not.shape[0])\n",
    "\n",
    "    accuracy = np.sum(sums) / np.sum(sizes)\n",
    "    print(\"Test Loss: {:.6f}\\t\".format(np.mean(test_losses)),\n",
    "          \"Accuracy: {:.3f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:52:42.361487Z",
     "start_time": "2020-10-28T07:52:42.322157Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpamClassifier(\n",
      "  (embedding): Embedding(5000, 100)\n",
      "  (lstm): LSTM(100, 8, num_layers=2, batch_first=True, dropout=0.2)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n",
      "\n",
      "SpamClassifier2(\n",
      "  (embedding): Embedding(5000, 100)\n",
      "  (lstm): LSTM(100, 8, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=16, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab)\n",
    "embedding_dim = 100 # int(vocab_size ** 0.25) # 8\n",
    "hidden_dim = 8\n",
    "output_size = 1\n",
    "n_layers = 2\n",
    "\n",
    "# LSTM\n",
    "net = SpamClassifier(vocab_size, embedding_dim, hidden_dim,\n",
    "                     output_size, n_layers, 0.2, 0.5)\n",
    "net = net.to(device)\n",
    "print(net)\n",
    "print()\n",
    "\n",
    "# Bi-LSTM\n",
    "net2 = SpamClassifier2(vocab_size, embedding_dim, hidden_dim,\n",
    "                     output_size, n_layers, 0.2, 0.5)\n",
    "net2 = net2.to(device)\n",
    "print(net2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:52:43.808190Z",
     "start_time": "2020-10-28T07:52:43.802262Z"
    }
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "epochs = 10\n",
    "lr=0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:53:42.722982Z",
     "start_time": "2020-10-28T07:52:49.806074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Batch number: 20, Training: Loss: 0.1635, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.3579, Accuracy: 0.8438\n",
      "Batch number: 60, Training: Loss: 0.1673, Accuracy: 0.9375\n",
      "Batch number: 80, Training: Loss: 0.2966, Accuracy: 0.9062\n",
      "Batch number: 100, Training: Loss: 0.0718, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.1166, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.0209, Accuracy: 1.0000\n",
      "Epoch : 001, Training: Loss: 0.2296, Accuracy: 91.8098%\n",
      "Epoch: 2/10\n",
      "Batch number: 20, Training: Loss: 0.0104, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0345, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.2216, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 0.1836, Accuracy: 0.8750\n",
      "Batch number: 120, Training: Loss: 0.1081, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.0155, Accuracy: 1.0000\n",
      "Epoch : 002, Training: Loss: 0.0683, Accuracy: 98.1023%\n",
      "Epoch: 3/10\n",
      "Batch number: 20, Training: Loss: 0.0678, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.0371, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0182, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0517, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 0.0388, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0235, Accuracy: 1.0000\n",
      "Epoch : 003, Training: Loss: 0.0602, Accuracy: 98.5417%\n",
      "Epoch: 4/10\n",
      "Batch number: 20, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0252, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0324, Accuracy: 0.9688\n",
      "Batch number: 80, Training: Loss: 0.0130, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0055, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.1401, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.1285, Accuracy: 0.9375\n",
      "Epoch : 004, Training: Loss: 0.0388, Accuracy: 98.9413%\n",
      "Epoch: 5/10\n",
      "Batch number: 20, Training: Loss: 0.0951, Accuracy: 0.9375\n",
      "Batch number: 40, Training: Loss: 0.0294, Accuracy: 0.9688\n",
      "Batch number: 60, Training: Loss: 0.0148, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0478, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 0.0186, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0014, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0163, Accuracy: 1.0000\n",
      "Epoch : 005, Training: Loss: 0.0405, Accuracy: 98.7415%\n",
      "Epoch: 6/10\n",
      "Batch number: 20, Training: Loss: 0.0621, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.1227, Accuracy: 0.9688\n",
      "Batch number: 60, Training: Loss: 0.0028, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0309, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 0.0108, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0058, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0034, Accuracy: 1.0000\n",
      "Epoch : 006, Training: Loss: 0.0318, Accuracy: 99.0212%\n",
      "Epoch: 7/10\n",
      "Batch number: 20, Training: Loss: 0.2517, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.0260, Accuracy: 0.9688\n",
      "Batch number: 60, Training: Loss: 0.0113, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0102, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0021, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0276, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0227, Accuracy: 1.0000\n",
      "Epoch : 007, Training: Loss: 0.0320, Accuracy: 99.1610%\n",
      "Epoch: 8/10\n",
      "Batch number: 20, Training: Loss: 0.1232, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0036, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0008, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0009, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0230, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0022, Accuracy: 1.0000\n",
      "Epoch : 008, Training: Loss: 0.0263, Accuracy: 99.2409%\n",
      "Epoch: 9/10\n",
      "Batch number: 20, Training: Loss: 0.0170, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0033, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0053, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0082, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0157, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0092, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0497, Accuracy: 0.9688\n",
      "Epoch : 009, Training: Loss: 0.0397, Accuracy: 99.0212%\n",
      "Epoch: 10/10\n",
      "Batch number: 20, Training: Loss: 0.0337, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.0081, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0010, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0031, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0642, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 0.2182, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.0625, Accuracy: 0.9688\n",
      "Epoch : 010, Training: Loss: 0.0304, Accuracy: 99.0012%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "train(net, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:54:35.059967Z",
     "start_time": "2020-10-28T07:53:42.726280Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10\n",
      "Batch number: 20, Training: Loss: 0.0140, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0124, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0990, Accuracy: 0.9375\n",
      "Batch number: 100, Training: Loss: 0.2961, Accuracy: 0.9062\n",
      "Batch number: 120, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0280, Accuracy: 1.0000\n",
      "Epoch : 001, Training: Loss: 0.0412, Accuracy: 98.7615%\n",
      "Epoch: 2/10\n",
      "Batch number: 20, Training: Loss: 0.0062, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0117, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0178, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0089, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0596, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 0.1348, Accuracy: 0.9375\n",
      "Batch number: 140, Training: Loss: 0.0874, Accuracy: 0.9688\n",
      "Epoch : 002, Training: Loss: 0.0398, Accuracy: 98.8813%\n",
      "Epoch: 3/10\n",
      "Batch number: 20, Training: Loss: 0.0343, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0368, Accuracy: 0.9688\n",
      "Batch number: 60, Training: Loss: 0.0255, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0079, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0123, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0138, Accuracy: 1.0000\n",
      "Epoch : 003, Training: Loss: 0.0374, Accuracy: 98.9213%\n",
      "Epoch: 4/10\n",
      "Batch number: 20, Training: Loss: 0.0073, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0067, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0078, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0582, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 0.0430, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.0145, Accuracy: 1.0000\n",
      "Epoch : 004, Training: Loss: 0.0474, Accuracy: 98.6217%\n",
      "Epoch: 5/10\n",
      "Batch number: 20, Training: Loss: 0.0935, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.0434, Accuracy: 0.9688\n",
      "Batch number: 60, Training: Loss: 0.0785, Accuracy: 0.9688\n",
      "Batch number: 80, Training: Loss: 0.0167, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0267, Accuracy: 0.9688\n",
      "Batch number: 120, Training: Loss: 0.1638, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.0174, Accuracy: 1.0000\n",
      "Epoch : 005, Training: Loss: 0.0469, Accuracy: 98.5218%\n",
      "Epoch: 6/10\n",
      "Batch number: 20, Training: Loss: 0.0160, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0059, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0227, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0656, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 0.0022, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0640, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.0125, Accuracy: 1.0000\n",
      "Epoch : 006, Training: Loss: 0.0373, Accuracy: 98.7016%\n",
      "Epoch: 7/10\n",
      "Batch number: 20, Training: Loss: 0.0590, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.0039, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.1527, Accuracy: 0.9375\n",
      "Batch number: 80, Training: Loss: 0.0559, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 0.0084, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0206, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0159, Accuracy: 1.0000\n",
      "Epoch : 007, Training: Loss: 0.0404, Accuracy: 98.7215%\n",
      "Epoch: 8/10\n",
      "Batch number: 20, Training: Loss: 0.0072, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0960, Accuracy: 0.9688\n",
      "Batch number: 60, Training: Loss: 0.0221, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0086, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0905, Accuracy: 0.9375\n",
      "Batch number: 120, Training: Loss: 0.0098, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.0894, Accuracy: 0.9688\n",
      "Epoch : 008, Training: Loss: 0.0361, Accuracy: 98.9213%\n",
      "Epoch: 9/10\n",
      "Batch number: 20, Training: Loss: 0.0046, Accuracy: 1.0000\n",
      "Batch number: 40, Training: Loss: 0.0518, Accuracy: 0.9688\n",
      "Batch number: 60, Training: Loss: 0.0235, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0455, Accuracy: 0.9688\n",
      "Batch number: 100, Training: Loss: 0.0100, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0528, Accuracy: 0.9688\n",
      "Batch number: 140, Training: Loss: 0.0087, Accuracy: 1.0000\n",
      "Epoch : 009, Training: Loss: 0.0396, Accuracy: 98.6416%\n",
      "Epoch: 10/10\n",
      "Batch number: 20, Training: Loss: 0.0473, Accuracy: 0.9688\n",
      "Batch number: 40, Training: Loss: 0.0118, Accuracy: 1.0000\n",
      "Batch number: 60, Training: Loss: 0.0106, Accuracy: 1.0000\n",
      "Batch number: 80, Training: Loss: 0.0272, Accuracy: 1.0000\n",
      "Batch number: 100, Training: Loss: 0.0070, Accuracy: 1.0000\n",
      "Batch number: 120, Training: Loss: 0.0150, Accuracy: 1.0000\n",
      "Batch number: 140, Training: Loss: 0.1807, Accuracy: 0.9375\n",
      "Epoch : 010, Training: Loss: 0.0405, Accuracy: 98.7215%\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net2.parameters(), lr=lr)\n",
    "\n",
    "train(net2, criterion, optimizer, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-28T07:54:35.460818Z",
     "start_time": "2020-10-28T07:54:35.065206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LSTM\n",
      "Test Loss: 0.096052\t Accuracy: 0.978\n",
      "Using Bi-LSTM\n",
      "Test Loss: 0.104543\t Accuracy: 0.978\n"
     ]
    }
   ],
   "source": [
    "print(\"Using LSTM\")\n",
    "eval(net, criterion)\n",
    "print(\"Using Bi-LSTM\")\n",
    "eval(net2, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**More to Improve...**  \n",
    "- activation function : `nn.Sigmoid()`외에 `F.log_softmax()` 등 사용 가능  \n",
    "\n",
    "- validation set 나눠서 가장 성능 좋은 모델 저장해두기  \n",
    "\n",
    "- epoch 수 늘리기, dropout rate 달리하기  \n",
    "\n",
    "- loss function : `BCELoss`외에 `NLLLoss`, `CrossEntropyLoss` 등 사용 가능"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
