{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "중간과제_NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GplcT7Ax-G9_"
      },
      "source": [
        "# Natural Language Processing Assignment: Spam Filter\n",
        "\n",
        "## 1) Import necessary libs and datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCsWh5V_-G-B"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0c7LyZx-G-J",
        "outputId": "3379e00a-55bc-402a-fa69-79a426c4d8c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "del data['Unnamed: 2']\n",
        "del data['Unnamed: 3']\n",
        "del data['Unnamed: 4']\n",
        "\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "data['text'] = data['v2']\n",
        "data['isSpam'] = data['v1']\n",
        "\n",
        "del data['v1'], data['v2']\n",
        "\n",
        "print(f'Data Shape: {data.shape}')\n",
        "# imbalanced data\n",
        "print(data['isSpam'].value_counts())\n",
        "data.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shape: (5572, 2)\n",
            "0    4825\n",
            "1     747\n",
            "Name: isSpam, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>isSpam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  isSpam\n",
              "0  Go until jurong point, crazy.. Available only ...       0\n",
              "1                      Ok lar... Joking wif u oni...       0\n",
              "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
              "3  U dun say so early hor... U c already then say...       0\n",
              "4  Nah I don't think he goes to usf, he lives aro...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gB0qDCOb-G-O"
      },
      "source": [
        "## 2) train, test split\n",
        "### 평가에 사용할 예정이니 트레인, 테스트 스플릿 코드는 그대로 유지시켜주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwbaeB6U-G-O",
        "outputId": "ee0470f9-23d2-435f-9cd4-c49ea9461c08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = data['text'], data['isSpam']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,\n",
        "                                                   stratify=y, test_size=0.1)\n",
        "\n",
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5014 558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ3js4z6-G-R",
        "outputId": "56e31b58-0ef7-4fb8-f8c4-4a68395935d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8jODRny-G-V"
      },
      "source": [
        "## 3-1) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSv_KajI-G-V"
      },
      "source": [
        "ref: https://towardsdatascience.com/text-preprocessing-for-data-scientist-3d2419c8199d"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxXd89AVvmny"
      },
      "source": [
        "idx = [i for i,item in enumerate(X_train) if len(item.split())<3]\n",
        "X_train = [x for i,x in enumerate(X_train) if i not in idx]\n",
        "y_train = [x for i,x in enumerate(y_train) if i not in idx]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yw6ntJB2Y14"
      },
      "source": [
        "idx2 = [i for i,item in enumerate(X_test) if len(item.split())<3]\n",
        "X_test = [x for i,x in enumerate(X_test) if i not in idx2]\n",
        "y_test = [x for i,x in enumerate(y_test) if i not in idx2]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsm3GJ1W-G-W"
      },
      "source": [
        "p = re.compile(r'(.)\\1{3,}', re.IGNORECASE)\n",
        "\n",
        "def preprocess(word: str) -> str:\n",
        "    word = word.lower().strip()\n",
        "    word = re.sub(r\"\\d+|\\\\\", \"\", word)\n",
        "    word = re.sub(\"[\"+string.punctuation+\"]\", \"\", word)\n",
        "    word = p.sub(r'\\1\\1',word)\n",
        "    word = str(TextBlob(word).correct()) \n",
        "    return(word)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4k_0_dDnG2On"
      },
      "source": [
        "sent_clean = [preprocess(sent) for sent in X_train]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0booCFFf2ji6"
      },
      "source": [
        "sent_clean2 = [preprocess(sent) for sent in X_test]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-iCGRcI-G-Z"
      },
      "source": [
        "## 3-2) Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X3pWnquE-G-Z"
      },
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5IAPwLB-G-d"
      },
      "source": [
        "def tokenize(word: str) -> list:\n",
        "    tokens = word_tokenize(word)\n",
        "    word = [item for item in tokens if item not in stop_words] #Stop words removal\n",
        "    return(word)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4R4DxCAp-G-f"
      },
      "source": [
        "word_clean = [tokenize(sent) for sent in sent_clean]\n",
        "word_clean = sum(word_clean, [])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulKv8XSM2olz"
      },
      "source": [
        "word_clean2 = [tokenize(sent) for sent in sent_clean2]\n",
        "word_clean2 = sum(word_clean2, [])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oS88iSa3-G-j"
      },
      "source": [
        "## 3-3) Build Vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ba87-L2-G-j"
      },
      "source": [
        "from collections import Counter\n",
        "vocab_count = Counter(word_clean)\n",
        "vocab_count = vocab_count.most_common(len(vocab_count))\n",
        "vocab_to_int = {word: index+2 for index, (word, count) in enumerate(vocab_count)}\n",
        "vocab_to_int.update({'padding__idx':0})\n",
        "vocab_to_int.update({'unk_idx':1})"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qB-Jz2j3Iis"
      },
      "source": [
        "vocab_count2 = Counter(word_clean2)\n",
        "vocab_count2 = vocab_count2.most_common(len(vocab_count2))\n",
        "vocab_to_int2 = {word: index+2 for index, (word, count) in enumerate(vocab_count2)}\n",
        "vocab_to_int2.update({'padding__idx':0})\n",
        "vocab_to_int2.update({'unk_idx':1})"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxFDDqB8-G-m"
      },
      "source": [
        "#### 여기서 ```padding_idx```는 패딩에 쓰이는 인덱스, ```unk_idx```는 unknown token을 의미합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMfOcyx8-G-n"
      },
      "source": [
        "### 3-4) toTensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yW3Crjdg-G-n"
      },
      "source": [
        "from torch.autograd import Variable"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7pfSusx-G-q",
        "outputId": "eafd596c-ff0a-4fd5-f3b8-370b2d5ee609",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        }
      },
      "source": [
        "# Tokenize & Vectorize sequences\n",
        "vectorized_seqs = []\n",
        "vectorized_seqs2 = []\n",
        "for seq in sent_clean: \n",
        "    vectorized_seqs.append([vocab_to_int.get(word,1) for word in seq.split()])\n",
        "\n",
        "for seq in sent_clean2: \n",
        "    vectorized_seqs2.append([vocab_to_int2.get(word,1) for word in seq.split()])\n",
        "\n",
        "# Save the lengths of sequences\n",
        "seq_lengths = torch.LongTensor(list(map(len, vectorized_seqs)))\n",
        "seq_lengths2 = torch.LongTensor(list(map(len, vectorized_seqs2)))\n",
        "\n",
        "# Add padding(0)\n",
        "seq_tensor = Variable(torch.zeros((len(vectorized_seqs), seq_lengths.max()))).long()\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs, seq_lengths)):\n",
        "    seq_tensor[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "seq_tensor2 = Variable(torch.zeros((len(vectorized_seqs2), seq_lengths2.max()))).long()\n",
        "for idx, (seq, seqlen) in enumerate(zip(vectorized_seqs2, seq_lengths2)):\n",
        "    seq_tensor2[idx, :seqlen] = torch.LongTensor(seq)\n",
        "\n",
        "print(seq_lengths.max()) # tensor(30772)\n",
        "print(seq_tensor[0]) # tensor([ 20,  77, 666,  ...,   0,   0,   0])\n",
        "print(seq_lengths[0]) # tensor(412)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(171)\n",
            "tensor([ 48,   1,   1,  63,   1,   1,   1, 492,   1, 127,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
            "          0,   0,   0])\n",
            "tensor(10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRsF36fS-G-x"
      },
      "source": [
        "## 4) DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhCijC84VGRi"
      },
      "source": [
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index]\n",
        "    subset_seq_lengths = self.seq_lengths[index]\n",
        "    subset_label_tensor = self.label_tensor[index]\n",
        "\n",
        "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
        "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
        "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeP_gmoyVKFN",
        "outputId": "63a702a5-674e-489a-dae8-642c9aa1e13b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# shuffle data\n",
        "label = np.array(y_train)\n",
        "label = torch.as_tensor(label, dtype=torch.int16)\n",
        "shuffled_idx = torch.randperm(len(label))\n",
        "\n",
        "seq_tensor = seq_tensor[shuffled_idx]\n",
        "seq_lengths = seq_lengths[shuffled_idx]\n",
        "label = label[shuffled_idx]\n",
        "\n",
        "# divide train data into 2 sets\n",
        "PCT_TRAIN = 0.7 # 70% of data will be train set \n",
        "PCT_VALID = 0.3 # 30% of data will be validation set\n",
        "\n",
        "length = len(label)\n",
        "train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
        "train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
        "train_label = label[:int(length*PCT_TRAIN)]\n",
        "\n",
        "valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
        "\n",
        "label2 = np.array(y_test)\n",
        "label2 = torch.as_tensor(label2, dtype=torch.int16)\n",
        "shuffled_idx2 = torch.randperm(len(label2))\n",
        "\n",
        "seq_tensor2 = seq_tensor2[shuffled_idx2]\n",
        "seq_lengths2 = seq_lengths2[shuffled_idx2]\n",
        "label2 = label2[shuffled_idx2]\n",
        "\n",
        "test_seq_tensor = seq_tensor2[:40]\n",
        "test_seq_lengths = seq_lengths2[:40]\n",
        "test_label = label2[:40]\n",
        "\n",
        "print(train_seq_tensor.shape) \n",
        "print(valid_seq_tensor.shape) \n",
        "print(test_seq_tensor.shape) \n",
        "\n",
        "batch_size = 80\n",
        "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
        "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
        "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3458, 171])\n",
            "torch.Size([1482, 171])\n",
            "torch.Size([40, 70])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-EeA8RvVOiH"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SpamHamLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers,\\\n",
        "                 drop_lstm=0.1, drop_out = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_lstm, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "\n",
        "        # embeddings\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "                \n",
        "        # pack, remove pads\n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "        \n",
        "        # lstm\n",
        "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
        " \n",
        "        # unpack, recover padded sequence\n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "       \n",
        "        # collect the last output in each batch\n",
        "        last_idxs = (input_sizes - 1).to(device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
        "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output).squeeze()\n",
        "               \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJQThrP6-Xo-"
      },
      "source": [
        "## 5) Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGF1aQiLcnKs",
        "outputId": "dae732f5-c727-4c16-9f4b-68dcff6ad1b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Instantiate the model w/ hyperparams\n",
        "\n",
        "vocab_size = len(vocab_to_int)\n",
        "embedding_dim = int(vocab_size ** 0.25)\n",
        "hidden_dim = 15\n",
        "output_size = 1\n",
        "n_layers = 2\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" \n",
        "net = SpamHamLSTM(vocab_size, embedding_dim, hidden_dim, output_size, n_layers, \\\n",
        "                 0.2, 0.2)\n",
        "net = net.to(device)\n",
        "print(net)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SpamHamLSTM(\n",
            "  (embedding): Embedding(5935, 8)\n",
            "  (lstm): LSTM(8, 15, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=15, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAkaofl_nWzZ"
      },
      "source": [
        "criterion = nn.BCELoss()\n",
        "\n",
        "lr=0.03\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,\\\n",
        "                                                       mode = 'min', \\\n",
        "                                                      factor = 0.5,\\\n",
        "                                                      patience = 2)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tw54mmJYGJu",
        "outputId": "fcf2463b-2fc3-4cea-bf30-81b9f863adff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# training params\n",
        "epochs = 6 \n",
        "counter = 0\n",
        "print_every = 10\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "val_losses = []\n",
        "for e in range(epochs):\n",
        "\n",
        "    scheduler.step(e)\n",
        "\n",
        "    for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "        counter += 1\n",
        "               \n",
        "        seq_tensor = seq_tensor.to(device)\n",
        "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "        label = label.to(device)\n",
        " \n",
        "        # get the output from the model\n",
        "        output = net(seq_tensor, seq_tensor_lengths)\n",
        "    \n",
        "        # get the loss and backprop\n",
        "        loss = criterion(output, label.float())\n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()\n",
        "        \n",
        "        # prevent the exploding gradient\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            \n",
        "            val_losses_in_itr = []\n",
        "            sums = []\n",
        "            sizes = []\n",
        "            \n",
        "            net.eval()\n",
        "            \n",
        "            for seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "                seq_tensor = seq_tensor.to(device)\n",
        "                seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "                label = label.to(device)\n",
        "                output = net(seq_tensor, seq_tensor_lengths)\n",
        "                \n",
        "                # losses\n",
        "                val_loss = criterion(output, label.float())     \n",
        "                val_losses_in_itr.append(val_loss.item())\n",
        "                \n",
        "                # accuracy\n",
        "                binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "                right_or_not = torch.eq(binary_output, label)\n",
        "                sums.append(torch.sum(right_or_not).float().item())\n",
        "                sizes.append(right_or_not.shape[0])\n",
        "            \n",
        "            accuracy = sum(sums) / sum(sizes)\n",
        "            \n",
        "            net.train()\n",
        "            print(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
        "                  \"Steps: {:3d}\\t\".format(counter),\n",
        "                  \"Loss: {:.6f}\\t\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\\t\".format(np.mean(val_losses_in_itr)),\n",
        "                  \"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  1/ 6\t Steps:  10\t Loss: 0.386192\t Val Loss: 0.396962\t Accuracy: 0.866\n",
            "Epoch:  1/ 6\t Steps:  20\t Loss: 0.459664\t Val Loss: 0.389783\t Accuracy: 0.866\n",
            "Epoch:  1/ 6\t Steps:  30\t Loss: 0.259173\t Val Loss: 0.363261\t Accuracy: 0.866\n",
            "Epoch:  1/ 6\t Steps:  40\t Loss: 0.270328\t Val Loss: 0.272558\t Accuracy: 0.866\n",
            "Epoch:  2/ 6\t Steps:  50\t Loss: 0.237449\t Val Loss: 0.182138\t Accuracy: 0.955\n",
            "Epoch:  2/ 6\t Steps:  60\t Loss: 0.098895\t Val Loss: 0.166708\t Accuracy: 0.938\n",
            "Epoch:  2/ 6\t Steps:  70\t Loss: 0.069132\t Val Loss: 0.137469\t Accuracy: 0.959\n",
            "Epoch:  2/ 6\t Steps:  80\t Loss: 0.032315\t Val Loss: 0.121557\t Accuracy: 0.962\n",
            "Epoch:  3/ 6\t Steps:  90\t Loss: 0.112846\t Val Loss: 0.137898\t Accuracy: 0.965\n",
            "Epoch:  3/ 6\t Steps: 100\t Loss: 0.029517\t Val Loss: 0.120280\t Accuracy: 0.957\n",
            "Epoch:  3/ 6\t Steps: 110\t Loss: 0.101225\t Val Loss: 0.105226\t Accuracy: 0.968\n",
            "Epoch:  3/ 6\t Steps: 120\t Loss: 0.025927\t Val Loss: 0.090130\t Accuracy: 0.970\n",
            "Epoch:  3/ 6\t Steps: 130\t Loss: 0.065104\t Val Loss: 0.095164\t Accuracy: 0.971\n",
            "Epoch:  4/ 6\t Steps: 140\t Loss: 0.004377\t Val Loss: 0.090959\t Accuracy: 0.976\n",
            "Epoch:  4/ 6\t Steps: 150\t Loss: 0.004902\t Val Loss: 0.090702\t Accuracy: 0.974\n",
            "Epoch:  4/ 6\t Steps: 160\t Loss: 0.049476\t Val Loss: 0.085908\t Accuracy: 0.973\n",
            "Epoch:  4/ 6\t Steps: 170\t Loss: 0.006222\t Val Loss: 0.089549\t Accuracy: 0.980\n",
            "Epoch:  5/ 6\t Steps: 180\t Loss: 0.002893\t Val Loss: 0.102246\t Accuracy: 0.976\n",
            "Epoch:  5/ 6\t Steps: 190\t Loss: 0.005587\t Val Loss: 0.094122\t Accuracy: 0.968\n",
            "Epoch:  5/ 6\t Steps: 200\t Loss: 0.008439\t Val Loss: 0.090165\t Accuracy: 0.973\n",
            "Epoch:  5/ 6\t Steps: 210\t Loss: 0.010922\t Val Loss: 0.095362\t Accuracy: 0.975\n",
            "Epoch:  5/ 6\t Steps: 220\t Loss: 0.002977\t Val Loss: 0.084887\t Accuracy: 0.978\n",
            "Epoch:  6/ 6\t Steps: 230\t Loss: 0.002453\t Val Loss: 0.088799\t Accuracy: 0.976\n",
            "Epoch:  6/ 6\t Steps: 240\t Loss: 0.002694\t Val Loss: 0.085750\t Accuracy: 0.976\n",
            "Epoch:  6/ 6\t Steps: 250\t Loss: 0.004127\t Val Loss: 0.089416\t Accuracy: 0.976\n",
            "Epoch:  6/ 6\t Steps: 260\t Loss: 0.001626\t Val Loss: 0.092032\t Accuracy: 0.975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfhDh9NL2Cr3",
        "outputId": "929c12ed-4265-4f63-9661-bc670f9df0f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "test_losses = []\n",
        "sums = []\n",
        "sizes = []\n",
        "\n",
        "net.eval()\n",
        "\n",
        "test_losses = []\n",
        "\n",
        "for seq_tensor, seq_tensor_lengths, label in iter(test_loader):\n",
        "\n",
        "    seq_tensor = seq_tensor.to(device)\n",
        "    seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "    label = label.to(device)\n",
        "    output = net(seq_tensor, seq_tensor_lengths)\n",
        "\n",
        "    # losses\n",
        "    test_loss = criterion(output, label.float())     \n",
        "    test_losses.append(test_loss.item())\n",
        "\n",
        "    # accuracy\n",
        "    binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "    right_or_not = torch.eq(binary_output, label)\n",
        "    sums.append(torch.sum(right_or_not).float().item())\n",
        "    sizes.append(right_or_not.shape[0])\n",
        "\n",
        "accuracy = np.sum(sums) / np.sum(sizes)\n",
        "print(\"Test Loss: {:.6f}\\t\".format(np.mean(test_losses)),\n",
        "      \"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Loss: 0.648150\t Accuracy: 0.875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "idV6Lo9k-G_H"
      },
      "source": [
        "# ref: https://github.com/sijoonlee/spam-ham-walkthrough/blob/master/walkthrough.ipynb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Xc--PVi-3gi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}