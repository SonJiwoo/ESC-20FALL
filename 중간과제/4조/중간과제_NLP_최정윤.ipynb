{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "수정.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02F1HDw6GU36"
      },
      "source": [
        "# Natural Language Processing Assignment: Spam Filter\n",
        "## Import necessary libs and datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMTuyiF5GU39"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "\n",
        "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
        "data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "from collections import Counter, OrderedDict"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Jx9Sg8EGU4G",
        "outputId": "be5e43c6-bb44-4579-c084-72f9e189b310",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "del data['Unnamed: 2']\n",
        "del data['Unnamed: 3']\n",
        "del data['Unnamed: 4']\n",
        "\n",
        "data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "data['text'] = data['v2']\n",
        "data['isSpam'] = data['v1']\n",
        "\n",
        "del data['v1'], data['v2']\n",
        "\n",
        "print(f'Data Shape: {data.shape}')\n",
        "# imbalanced data\n",
        "print(data['isSpam'].value_counts())\n",
        "data.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Shape: (5572, 2)\n",
            "0    4825\n",
            "1     747\n",
            "Name: isSpam, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>isSpam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  isSpam\n",
              "0  Go until jurong point, crazy.. Available only ...       0\n",
              "1                      Ok lar... Joking wif u oni...       0\n",
              "2  Free entry in 2 a wkly comp to win FA Cup fina...       1\n",
              "3  U dun say so early hor... U c already then say...       0\n",
              "4  Nah I don't think he goes to usf, he lives aro...       0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRwhhQP_GU4O"
      },
      "source": [
        "## train, test split\n",
        "### 평가에 사용할 예정이니 트레인, 테스트 스플릿 코드는 그대로 유지시켜주세요"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PAJu3PwGU4Q",
        "outputId": "41bdadca-9efc-471a-93e6-4d586546c9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = data['text'], data['isSpam']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0,\n",
        "                                                   stratify=y, test_size=0.1)\n",
        "\n",
        "print(len(X_train), len(X_test))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5014 558\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-3eREJ1JtgK",
        "outputId": "00d2e541-d239-41f9-db04-77416c0aeacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "X"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       Go until jurong point, crazy.. Available only ...\n",
              "1                           Ok lar... Joking wif u oni...\n",
              "2       Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3       U dun say so early hor... U c already then say...\n",
              "4       Nah I don't think he goes to usf, he lives aro...\n",
              "                              ...                        \n",
              "5567    This is the 2nd time we have tried 2 contact u...\n",
              "5568                Will Ì_ b going to esplanade fr home?\n",
              "5569    Pity, * was in mood for that. So...any other s...\n",
              "5570    The guy did some bitching but I acted like i'd...\n",
              "5571                           Rofl. Its true to its name\n",
              "Name: text, Length: 5572, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0mNWxtAGU4V"
      },
      "source": [
        "## Preprocessing\n",
        "### 텍스트 전처리함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVeUGOAaGU4W"
      },
      "source": [
        "def preprocess(string: str, *args, **kwargs):\n",
        "    string = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', string)\n",
        "    string = re.sub(r\"[^가-힣A-Za-z0-9()]\", \" \", string)     # 한글, 영문, 숫자, 괄호, 쉼표, 느낌표, 물음표, 작음따옴표, 역따옴표 제외한 나머지 모두 찾아서 공백(\" \")으로 바꾸기\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "    string = re.sub(r\",\", \" , \", string) \n",
        "    string = re.sub(r\"!\", \" ! \", string) \n",
        "    string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "    string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "    string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\"\\s'{2,}\", \"\\'\", string)\n",
        "    string = re.sub(r\"\\'\", \"\", string) \n",
        "    string = string.strip()                         \n",
        "\n",
        "\n",
        "    return string.lower()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AipvjzEWZrQa",
        "outputId": "0816f3f9-a92d-4397-d004-ee75ac2a6f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "preprocess('Helllllo World-!')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'hello world'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8bls7sxGU4a"
      },
      "source": [
        "#### 앞에서 보셨다시피 raw text를 그대로 사용하기엔 무리가 있습니다.(특수기호 및 불용어 문제 등)\n",
        "#### 따라서 전처리되지 않은 raw string을 전처리하는 함수를 만들어주세요. <br>\n",
        "```python\n",
        "preprocess('Helllllo World-!') = 'hello world'\n",
        "```\n",
        "<br>\n",
        "\n",
        "#### ```re``` library를 이용해서 전처리를 쉽게 할 수 있습니다.\n",
        "\n",
        "\n",
        "[re documentation](https://docs.python.org/3/library/re.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmy3It9dGU4a"
      },
      "source": [
        "## Tokenizing\n",
        "### 전처리된 텍스트를 토크나이징 해주는 함수입니다.\n",
        "#### ```SpaCy, nltk``` 등 영어 tokenizing 라이브러리를 쓰셔도 괜찮습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnNsnTzpGU4b"
      },
      "source": [
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize \n",
        "\n",
        "def tokenize(string: str, *args, **kwargs) -> list:\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\")) \n",
        "    word_tokens = word_tokenize(string) \n",
        "    filtered_text = [word for word in word_tokens if word not in stop_words] \n",
        "    return filtered_text "
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59Xz5BAgp01e",
        "outputId": "865ba944-6b05-43c0-d652-cc84654ea9d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTslQ2y9iEzT",
        "outputId": "85999380-9451-4f34-b50a-56344d589e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tokenize(preprocess('hello world!'))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'world']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9t-FgHIGU4e"
      },
      "source": [
        "<br>\n",
        "\n",
        "Ex) \n",
        "```python\n",
        "tokenize('hello world!',  *args, **kwargs) = ['hello', 'world']\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNDVWPZGGU4f"
      },
      "source": [
        "## Build Vocabulary\n",
        "### 토큰들을 이용해서 자주 등장한 순서대로 n개의 원소를 갖는 딕셔너리를 만들어주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDRn7AmjGU4f"
      },
      "source": [
        "from collections import Counter, OrderedDict\n",
        "\n",
        "def build_vocab(n, words, **kwargs):\n",
        "    wordfreq = dict(Counter(words).most_common(n-2))\n",
        "\n",
        "    vocab = OrderedDict()\n",
        "    vocab['padding_idx'] = 0\n",
        "    vocab['unk_idx'] = 1\n",
        "\n",
        "    idx = 2\n",
        "    for word in wordfreq.keys():\n",
        "      vocab[word] =  idx\n",
        "      idx += 1\n",
        "\n",
        "    return vocab"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXkmaApko77i",
        "outputId": "09187fc5-a74e-4f66-b5f5-322c68c6b020",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "vocab = build_vocab(4,tokenize(preprocess('hello world!')))\n",
        "vocab"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('padding_idx', 0), ('unk_idx', 1), ('hello', 2), ('world', 3)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYl7o4DlGU4i"
      },
      "source": [
        "<br>\n",
        "\n",
        "Ex) \n",
        "```python\n",
        "vocab = build_vocab(4, *args, **kwargs)\n",
        "vocab = {'padding_idx': 0, 'unk_idx': 1, 'hello': 2, 'world': 3}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-3BIZIKGU4j"
      },
      "source": [
        "#### 여기서 ```padding_idx```는 패딩에 쓰이는 인덱스, ```unk_idx```는 unknown token을 의미합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCXCM6e5GU4j"
      },
      "source": [
        "### toTensor\n",
        "#### 토큰들을 텐서로 바꿔주는 함수입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gW0O9VUqizR"
      },
      "source": [
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka28BKD8qSH4",
        "outputId": "0e9553b6-a2b3-4589-d083-12b1c69954da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "\n",
        "max_len = max(len(l) for l in X)\n",
        "print('최대 길이 : %d' % max_len)\n",
        "print('최소 길이 : %d' % min(len(l) for l in X))\n",
        "print('평균 길이 : %f' % (sum(map(len, X))/len(X)))\n",
        "plt.hist([len(s) for s in X], bins=50)\n",
        "plt.xlabel('length of sample')\n",
        "plt.ylabel('number of sample')\n",
        "plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최대 길이 : 910\n",
            "최소 길이 : 2\n",
            "평균 길이 : 80.118808\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAaFElEQVR4nO3dfZQddX3H8ffHBIIokoSkaciDG0oOlVqFuDwdqY2mxfBQgy0qqZaIqTm1KGlFMWiPwacajhaEVinBRCKlIEWUFFIwDYnUKiEJxCQ8lRWC2TSQ1YTwVB4C3/4xvzXDspuZ7O7ce3fv53XOnDvzm9+d+d7Zu/vd+c3M76eIwMzMbG9eU+8AzMys8TlZmJlZIScLMzMr5GRhZmaFnCzMzKzQ0HoHUIVRo0ZFS0tLvcMwMxtQ1q1b96uIGN3dukGZLFpaWli7dm29wzAzG1AkPdrTOjdDmZlZIScLMzMr5GRhZmaFnCzMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZoUH5BHdVWubd0m355gWn1jgSM7Pa8pmFmZkVcrIwM7NCThZmZlbIycLMzApVliwkLZa0XdKmbtadJykkjUrLknSZpDZJGyRNydWdJemhNM2qKl4zM+tZlWcWVwHTuxZKmgCcBPwyV3wyMDlNc4DLU92RwHzgOOBYYL6kERXGbGZm3agsWUTEHcCOblZdApwPRK5sBvDdyNwJDJc0Fng3sDwidkTETmA53SQgMzOrVk2vWUiaAWyNiJ93WTUO2JJbbk9lPZWbmVkN1eyhPEkHAp8la4KqYvtzyJqwmDhxYhW7MDNrWrU8s/gdYBLwc0mbgfHA3ZJ+G9gKTMjVHZ/Keip/lYhYGBGtEdE6enS3442bmVkv1SxZRMTGiPitiGiJiBayJqUpEfEYsBQ4K90VdTywKyK2AbcBJ0kakS5sn5TKzMyshqq8dfZa4GfAEZLaJc3eS/VlwMNAG3Al8NcAEbED+BKwJk1fTGVmZlZDlV2ziIiZBetbcvMBnNNDvcXA4n4NzszM9omf4DYzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr5GRhZmaFnCzMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr5GRhZmaFnCzMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZocqShaTFkrZL2pQr+5qkByRtkPQDScNz6y6Q1CbpQUnvzpVPT2VtkuZVFa+ZmfWsyjOLq4DpXcqWA2+OiLcA/wNcACDpSOBM4PfSe74laYikIcA3gZOBI4GZqa6ZmdVQZckiIu4AdnQp+1FE7E6LdwLj0/wM4LqIeD4iHgHagGPT1BYRD0fEC8B1qa6ZmdVQPa9ZfAT4jzQ/DtiSW9eeynoqfxVJcyStlbS2o6OjgnDNzJpXXZKFpM8Bu4Fr+mubEbEwIlojonX06NH9tVkzMwOG1nqHkj4MnAZMi4hIxVuBCblq41MZeyk3M7MaqemZhaTpwPnAeyLi2dyqpcCZkoZJmgRMBu4C1gCTJU2StD/ZRfCltYzZzMwqPLOQdC0wFRglqR2YT3b30zBguSSAOyPiryLiXknXA/eRNU+dExEvpe18HLgNGAIsjoh7q4rZzMy6V1myiIiZ3RQv2kv9rwBf6aZ8GbCsH0MzM7N95Ce4zcyskJOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCpZKFpBMlnZ3mR6eeYc3MrEkUJgtJ84HPkMbLBvYD/qXKoMzMrLGUObN4L/Ae4BmAiPhf4KAqgzIzs8ZSJlm8kEa0CwBJr6s2JDMzazRlksX1kq4Ahkv6KPCfwJXVhmVmZo2kcPCjiPi6pD8GngSOAD4fEcsrj8zMzBpGqZHyUnJwgjAza1I9JgtJT5GuU3RdBUREvKGyqMzMrKH0eM0iIg6KiDd0Mx1UJlFIWixpu6RNubKRkpZLeii9jkjlknSZpDZJGyRNyb1nVqr/kKRZff3AZma278o+lDdF0rmSPiHp6JLbvgqY3qVsHrAiIiYDK9IywMnA5DTNAS5P+x0JzAeOA44F5ncmGDMzq50yD+V9HlgCHAKMAq6S9HdF74uIO4AdXYpnpG2RXk/PlX83MneS3Xk1Fng3sDwidkTETrLrJl0TkJmZVazMBe4PAm+NiOcAJC0A1gNf7sX+xkTEtjT/GDAmzY8DtuTqtaeynspfRdIcsrMSJk6c2IvQeq9l3i3dlm9ecGpN4zAzq0qZZqj/BQ7ILQ8DtvZ1x/kH/fpDRCyMiNaIaB09enR/bdbMzCiXLHYB90q6StJ3gE3AE+mC9GX7uL/HU/MS6XV7Kt8KTMjVG5/Keio3M7MaKtMM9YM0dVrVh/0tBWYBC9LrTbnyj0u6juxi9q6I2CbpNuDvcxe1T2JPh4ZmZlYjZZ7gXlJUpzuSrgWmAqMktZPd1bSArPuQ2cCjwPtT9WXAKUAb8Cxwdtr3DklfAtakel+MiK4Xzc3MrGKFyULSacCXgDem+qUeyouImT2smtZN3QDO6WE7i4HFRXGamVl1yjRDfQP4U2Bj+qNuZmZNpswF7i3AJicKM7PmVebM4nxgmaQfA893FkbExZVFZWZmDaVMsvgK8DTZsxb7VxuOmZk1ojLJ4tCIeHPlkZiZWcMqc81imaSTKo/EzMwaVplk8THgVkn/J+lJSU9JerLqwMzMrHGUeSjvoFoEYmZmjavUsKqpu43J5DoUTF2Qm5lZEyjzBPdfAnPJOvFbDxwP/Ax4V7WhmZlZoyhzzWIucAzwaES8EzgaeKLSqMzMrKGUSRbP5QY+GhYRDwBHVBuWmZk1kjLXLNolDQd+CCyXtJOsx1gzM2sSZe6Gem+avVDSSuBg4NZKozIzs4ZS2Awl6XckDetcBFqAA6sMyszMGkuZaxbfB16SdDiwkGyY03+tNCozM2soZZLFyxGxG3gv8I8R8WlgbLVhmZlZIymTLF6UNJNszOybU9l+1YVkZmaNpkyyOBs4AfhKRDwiaRJwdbVhmZlZIylMFhFxX0ScGxHXpuVHIuKivuxU0t9KulfSJknXSjpA0iRJqyW1SfqepP1T3WFpuS2tb+nLvs3MbN+VObPoV5LGAecCrWmcjCHAmcBFwCURcTiwE5id3jIb2JnKL0n1zMyshmqeLJKhwGslDSW7DXcbWV9TN6T1S4DT0/yMtExaP02SahirmVnT6zFZSLo6vc7tzx1GxFbg68AvyZLELmAd8ES66wqgHRiX5scBW9J7d6f6h/RnTGZmtnd7O7N4m6RDgY9IGiFpZH7q7Q5Td+czgEnAocDrgOm93V5uu3MkrZW0tqOjo6+bMzOznL119/HPwArgMLL//PNNP5HKe+OPgEciogNA0o3A24Hhkoams4fxwNZUfyvZg4DtqdnqYODXXTcaEQvJHhqktbU1ehmbmZl1o8czi4i4LCLeBCyOiMMiYlJu6m2igKz56XhJB6ZrD9OA+4CVwBmpzizgpjS/NC2T1t8eEU4GZmY1VKYjwY9JeivwB6nojojY0NsdRsRqSTcAdwO7gXvIzghuAa6T9OVUtii9ZRFwtaQ2YAfZnVNmZlZDZUbKOxeYA9yYiq6RtDAi/rG3O42I+cD8LsUPA8d2U/c54H293ZeZmfVdmfEs/hI4LiKeAZB0Edmwqr1OFmZmNrCUec5CwEu55Zd45cVuMzMb5MqcWXwHWC3pB2n5dPZcTzAzsyZQ5gL3xZJWASemorMj4p5KozIzs4ZS5syCiLib7O4lMzNrQvXqG8rMzAYQJwszMyu012QhaYiklbUKxszMGtNek0VEvAS8LOngGsVjZmYNqMwF7qeBjZKWA890FkbEuZVFZWZmDaVMsriRPV19mJlZEyrznMUSSa8FJkbEgzWIyczMGkzh3VCS/gRYD9yalo+StLTqwMzMrHGUuXX2QrLeYJ8AiIj19H7gIzMzG4DKJIsXI2JXl7KXqwjGzMwaU5kL3PdK+nNgiKTJwLnAT6sNy8zMGkmZZPEJ4HPA88C1wG3Al6oMyvqmZd4t3ZZvXnBqjSMxs8GizN1QzwKfS4MeRUQ8VX1YZmbWSMrcDXWMpI3ABrKH834u6W3Vh2ZmZo2iTDPUIuCvI+K/ACSdSDYg0luqDMzMzBpHmbuhXupMFAAR8RNgd192Kmm4pBskPSDpfkknSBopabmkh9LriFRXki6T1CZpg6Qpfdm3mZntux7PLHJ/lH8s6Qqyi9sBfABY1cf9XgrcGhFnSNofOBD4LLAiIhZImgfMAz4DnAxMTtNxwOXpteH5QrOZDRZ7a4b6hy7L83Pz0dsdph5s3wF8GCAiXgBekDQDmJqqLSFLSJ8BZgDfjYgA7kxnJWMjYltvYzAzs33TY7KIiHdWtM9JQAfwHUlvBdYBc4ExuQTwGDAmzY8DtuTe357KXpEsJM0B5gBMnDixotDNzJpT4QVuScOBs4CWfP0+dFE+FJgCfCIiVku6lKzJ6TciIiTt09lLRCwEFgK0trb2+szHzMxerczdUMuAO4GN9E83H+1Ae0SsTss3kCWLxzublySNBban9VuBCbn3j09lZmZWI2WSxQER8cn+2mFEPCZpi6QjUpfn04D70jQLWJBeb0pvWQp8XNJ1ZBe2d/l6hZlZbZVJFldL+ihwM1mXHwBExI4+7PcTwDXpTqiHgbPJbuO9XtJs4FHg/anuMuAUoA14NtU1M7MaKpMsXgC+RtY/VOe1gKAP3ZSnbs5bu1k1rZu6AZzT232ZmVnflUkW5wGHR8Svqg7GzMwaU5knuDubf8zMrEmVObN4BlgvaSWvvGbR21tnzcxsgCmTLH6YJjMza1JlxrNYUotAzMyscZV5gvsRuukLKiJ6fTeUmZkNLGWaofK3uB4AvA8YWU04ZmbWiArvhoqIX+emrRHxDcB9bJuZNZEyzVD5wYZeQ3amUeaMxMzMBokyf/Tz41rsBjazpysOMzNrAmXuhqpqXAszMxsgyjRDDQP+jFePZ/HF6sIyM7NGUqYZ6iZgF9mIds8X1DUzs0GoTLIYHxHTK4/EzMwaVpmOBH8q6fcrj8TMzBpWmTOLE4EPpye5nwdENszEWyqNbBBrmXdLt+WbF/jxFTNrTGWSxcmVR2FmZg2tzK2zj9YiEDMza1xlrlmYmVmTq1uykDRE0j2Sbk7LkyStltQm6XuS9k/lw9JyW1rfUq+YzcyaVT3PLOYC9+eWLwIuiYjDgZ3A7FQ+G9iZyi9J9czMrIbqkiwkjSfrufbbaVnAu4AbUpUlwOlpfkZaJq2fluqbmVmN1OvM4hvA+cDLafkQ4ImI2J2W24FxaX4csAUgrd+V6r+CpDmS1kpa29HRUWXsZmZNp+ZdjUs6DdgeEeskTe2v7UbEQmAhQGtr66tG9huMenpew8ysv9VjXIq3A++RdArZyHtvAC4Fhksams4exgNbU/2twASgXdJQ4GDg17UP28ysedW8GSoiLoiI8RHRApwJ3B4RHwRWAmekarPIOjAEWJqWSetvj4imOHMwM2sUjfScxWeAT0pqI7smsSiVLwIOSeWfBObVKT4zs6ZV1+FRI2IVsCrNPwwc202d54D31TQwMzN7hUY6szAzswblZGFmZoWcLMzMrJCThZmZFXKyMDOzQk4WZmZWqK63zlo57tbDzOrNZxZmZlbIycLMzAq5GaqBuLnJzBqVzyzMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZmJlZIScLMzMr5GRhZmaFnCzMzKxQzZOFpAmSVkq6T9K9kuam8pGSlkt6KL2OSOWSdJmkNkkbJE2pdcxmZs2uHmcWu4HzIuJI4HjgHElHAvOAFRExGViRlgFOBianaQ5wee1DNjNrbjVPFhGxLSLuTvNPAfcD44AZwJJUbQlwepqfAXw3MncCwyWNrXHYZmZNra7XLCS1AEcDq4ExEbEtrXoMGJPmxwFbcm9rT2VdtzVH0lpJazs6OiqL2cysGdUtWUh6PfB94G8i4sn8uogIIPZlexGxMCJaI6J19OjR/RipmZnVJVlI2o8sUVwTETem4sc7m5fS6/ZUvhWYkHv7+FRmZmY1Uo+7oQQsAu6PiItzq5YCs9L8LOCmXPlZ6a6o44FdueYqMzOrgXoMfvR24C+AjZLWp7LPAguA6yXNBh4F3p/WLQNOAdqAZ4GzaxuumZnVPFlExE8A9bB6Wjf1Azin0qC68Ih1Zmav5Ce4zcyskJOFmZkVcrIwM7NCThZmZlbIycLMzAo5WZiZWSEnCzMzK+RkYWZmhZwszMyskJOFmZkVcrIwM7NCThZmZlaoHr3OWp3srYPEzQtOrWEkZjbQ+MzCzMwKOVmYmVkhJwszMyvkZGFmZoWcLMzMrJCThZmZFfKtswb0fFutb6k1MxhAyULSdOBSYAjw7YhYUOeQmlp/JRcnKbOBYUAkC0lDgG8Cfwy0A2skLY2I++ob2eC3twf59qW+//ibDWwDIlkAxwJtEfEwgKTrgBmAk8UA0V9JZ1/1lKRq8TS7E6cNJgMlWYwDtuSW24Hj8hUkzQHmpMWnJT3Yy32NAn7Vy/cONgP+WOiifntPvx2L3sTUYAb896IfDbZj8caeVgyUZFEoIhYCC/u6HUlrI6K1H0Ia8Hws9vCx2MPHYo9mOhYD5dbZrcCE3PL4VGZmZjUwUJLFGmCypEmS9gfOBJbWOSYzs6YxIJqhImK3pI8Dt5HdOrs4Iu6taHd9bsoaRHws9vCx2MPHYo+mORaKiHrHYGZmDW6gNEOZmVkdOVmYmVkhJ4tE0nRJD0pqkzSv3vFUTdIESSsl3SfpXklzU/lIScslPZReR6RySbosHZ8NkqbU9xP0P0lDJN0j6ea0PEnS6vSZv5durkDSsLTclta31DPu/iZpuKQbJD0g6X5JJzTr90LS36bfj02SrpV0QLN+L5wseEV3IicDRwIzJR1Z36gqtxs4LyKOBI4HzkmfeR6wIiImAyvSMmTHZnKa5gCX1z7kys0F7s8tXwRcEhGHAzuB2al8NrAzlV+S6g0mlwK3RsTvAm8lOyZN972QNA44F2iNiDeT3VxzJs36vYiIpp+AE4DbcssXABfUO64aH4ObyPreehAYm8rGAg+m+SuAmbn6v6k3GCayZ3dWAO8CbgZE9mTu0K7fEbK78k5I80NTPdX7M/TTcTgYeKTr52nG7wV7eo4YmX7ONwPvbsbvRUT4zCLprjuRcXWKpebS6fLRwGpgTERsS6seA8ak+cF+jL4BnA+8nJYPAZ6IiN1pOf95f3Ms0vpdqf5gMAnoAL6TmuS+Lel1NOH3IiK2Al8HfglsI/s5r6M5vxdOFs1O0uuB7wN/ExFP5tdF9i/SoL+3WtJpwPaIWFfvWBrAUGAKcHlEHA08w54mJ6CpvhcjyDosnQQcCrwOmF7XoOrIySLTlN2JSNqPLFFcExE3puLHJY1N68cC21P5YD5GbwfeI2kzcB1ZU9SlwHBJnQ+u5j/vb45FWn8w8OtaBlyhdqA9Ilan5RvIkkczfi/+CHgkIjoi4kXgRrLvSjN+L5wskqbrTkSSgEXA/RFxcW7VUmBWmp9Fdi2js/ysdPfL8cCuXLPEgBYRF0TE+IhoIfvZ3x4RHwRWAmekal2PRecxOiPVHxT/aUfEY8AWSUekomlkQwE03feCrPnpeEkHpt+XzmPRdN8LwBe4OyfgFOB/gF8An6t3PDX4vCeSNSVsANan6RSyNtYVwEPAfwIjU32R3TH2C2Aj2R0idf8cFRyXqcDNaf4w4C6gDfg3YFgqPyAtt6X1h9U77n4+BkcBa9N344fAiGb9XgBfAB4ANgFXA8Oa9Xvh7j7MzKyQm6HMzKyQk4WZmRVysjAzs0JOFmZmVsjJwszMCjlZ2KAi6ekKtnmUpFNyyxdK+lQftve+1Jvryv6JsNdxbJY0qp4x2MDhZGFW7CiyZ1D6y2zgoxHxzn7cplmlnCxs0JL0aUlr0jgLX0hlLem/+ivTOAU/kvTatO6YVHe9pK+lMQz2B74IfCCVfyBt/khJqyQ9LOncHvY/U9LGtJ2LUtnnyR6IXCTpa13qj5V0R9rPJkl/kMovl7Q2xfuFXP3Nkr6a6q+VNEXSbZJ+IemvUp2paZu3KBuv5Z8lver3XtKHJN2VtnVF6rbfbI96PxXoyVN/TsDT6fUkYCHZE8avIete+h1AC9lYHkeletcDH0rzm9jTxfQCYFOa/zDwT7l9XAj8lOxp3lFk/f/s1yWOQ8m6ixhN1jnf7cDpad0qunnSGTiP1HsA2dgJB6X5kbmyVcBb0vJm4GNp/hKyJ64PSvt8PJVPBZ4je+p4CLAcOCP3/lHAm4B/7/wMwLeAs+r9s/TUWJPPLGywOilN9wB3A79LNkAPZJ3DrU/z64AWScPJ/jj/LJX/a8H2b4mI5yPiV2Sd6o3psv4YYFVkndDtBq4hS1Z7swY4W9KFwO9HxFOp/P2S7k6f5ffIBujq1NmH2UZgdUQ8FREdwPPpMwHcFREPR8RLwLVkZzZ504C3AWskrU/LhxXEak1maHEVswFJwFcj4opXFGZjdzyfK3oJeG0vtt91G33+XYqIOyS9AzgVuErSxcB/AZ8CjomInZKuIuuDqGscL3eJ6eVcTF379Om6LGBJRFzQ189gg5fPLGywug34SBqvA0njJP1WT5Uj4gngKUnHpaIzc6ufImve2Rd3AX8oaVRq/58J/Hhvb5D0RrLmoyuBb5N1Df4GsjEldkkaQzaM6b46NvWo/BrgA8BPuqxfAZzReXyUjbf9xl7sxwYxn1nYoBQRP5L0JuBnWe/SPA18iOwsoCezgSslvUz2h31XKl8JzEtNNF8tuf9tkual94qs2eqmgrdNBT4t6cUU71kR8Yike8h6Pt0C/HeZ/XexBvgn4PAUzw+6xHqfpL8DfpQSyovAOcCjvdiXDVLuddYskfT6iHg6zc8jG0t6bp3D6hNJU4FPRcRp9Y7FBjafWZjtcaqkC8h+Lx4luwvKzPCZhZmZleAL3GZmVsjJwszMCjlZmJlZIScLMzMr5GRhZmaF/h+lenZwIbXhQQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULj7ToxMr0BE"
      },
      "source": [
        "def toTensor(max_len, words, vocab) -> torch.LongTensor:\n",
        "  vocab = vocab\n",
        "  max_len = max_len\n",
        "  tokenized = list(map(preprocess,words))\n",
        "  temp = []\n",
        "  for w in tokenized: #각 줄에서 1개씩 글자를 읽음\n",
        "      try:\n",
        "        temp.append(vocab[w]) # 글자를 해당되는 정수로 변환\n",
        "      except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
        "        temp.append(vocab['unk_idx']) # unk의 인덱스로 변환\n",
        "\n",
        "  if len(temp) < max_len: \n",
        "      temp += [vocab['padding_idx']] * (max_len - len(temp)) \n",
        "  \n",
        "  tensor = torch.FloatTensor(temp)\n",
        "  return tensor.type(torch.LongTensor)\n",
        "      "
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fch7M5xq8_03",
        "outputId": "106ff562-b981-4a6d-86c7-49f8110ee46d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "toTensor(5, ['hello', 'world!', 'yonsei'],vocab)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 3, 1, 0, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pFRVecSUGU4l"
      },
      "source": [
        "시퀀스의 max length가 5일 때 다음과 같습니다.\n",
        "<br>\n",
        "\n",
        "Ex)\n",
        "```python\n",
        "toTensor(5, ['hello', 'world!', 'yonsei']) = torch.LongTensor([2, 3, 1, 0, 0])\n",
        "```\n",
        "\n",
        "여기서 ```yonsei``` 단어는 아까 만든 단어장(vocab)에 포함되지 않은 단어로 ```unk_idx```로 처리됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqn9WNw4GU4m"
      },
      "source": [
        "### 위의 함수들을 이용하고 적절한 코드 및 parameter를 적용해서 \n",
        "### MailDataset과 train에 쓸 DataLoader를 만들어주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe-KXgFcGU4m"
      },
      "source": [
        "class MailDataset(Dataset):\n",
        "\n",
        "    def __init__(self):\n",
        "        urllib.request.urlretrieve(\"https://raw.githubusercontent.com/mohitgupta-omg/Kaggle-SMS-Spam-Collection-Dataset-/master/spam.csv\", filename=\"spam.csv\")\n",
        "        data = pd.read_csv('spam.csv', encoding='latin1')\n",
        "        del data['Unnamed: 2']\n",
        "        del data['Unnamed: 3']\n",
        "        del data['Unnamed: 4']\n",
        "\n",
        "        data['v1'] = data['v1'].replace(['ham','spam'],[0,1])\n",
        "        data['text'] = data['v2']\n",
        "        data['isSpam'] = data['v1']\n",
        "\n",
        "        del data['v1'], data['v2']\n",
        "        self.data = data\n",
        "        self.n = 20000\n",
        "        self.max_len = 80\n",
        "\n",
        "    # data cleansing\n",
        "    def preprocess(self):\n",
        "        x= self.data['text']\n",
        "        lst = []\n",
        "        for string in x:\n",
        "          string = re.sub(r'(\\w)\\1{2,}', r'\\1\\1', string)\n",
        "          string = re.sub(r\"[^가-힣A-Za-z()]\", \" \", string)     # 한글, 영문, 숫자, 괄호, 쉼표, 느낌표, 물음표, 작음따옴표, 역따옴표 제외한 나머지 모두 찾아서 공백(\" \")으로 바꾸기\n",
        "          string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "          string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "          string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "          string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "          string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "          string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "          string = re.sub(r\",\", \" , \", string) \n",
        "          string = re.sub(r\"!\", \" ! \", string) \n",
        "          string = re.sub(r\"\\(\", \" \\( \", string) \n",
        "          string = re.sub(r\"\\)\", \" \\) \", string) \n",
        "          string = re.sub(r\"\\?\", \" \\? \", string) \n",
        "          string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "          string = re.sub(r\"\\s'{2,}\", \"\\'\", string)\n",
        "          string = re.sub(r\"\\'\", \"\", string) \n",
        "          string = string.strip() \n",
        "          string = string.lower()\n",
        "          lst.append(string)                        \n",
        "\n",
        "        return lst\n",
        "\n",
        "    # Tokenizing\n",
        "    def tokenize(self, return_type='words') -> list:\n",
        "        return_type = return_type\n",
        "        words = []\n",
        "        sentences = []\n",
        "        for string in self.preprocess():\n",
        "          lst=[]\n",
        "          stop_words = set(stopwords.words(\"english\")) \n",
        "          word_tokens = word_tokenize(string) \n",
        "          for word in word_tokens:\n",
        "            if word not in stop_words:\n",
        "                if len(word)>=2:\n",
        "                    words.append(word)\n",
        "                    lst.append(word)\n",
        "          sentences.append(lst)\n",
        "            \n",
        "        if return_type == 'words':\n",
        "          return words\n",
        "        \n",
        "        if return_type == 'sentence':\n",
        "          return sentences\n",
        "\n",
        "    # Building a vocab\n",
        "    def build_vocab(self):\n",
        "        words = self.tokenize() \n",
        "        n = self.n \n",
        "        #words = sum(words, [])  \n",
        "        wordfreq = dict(Counter(words).most_common(n-2))\n",
        "\n",
        "        vocab = OrderedDict()\n",
        "        vocab['padding_idx'] = 0\n",
        "        vocab['unk_idx'] = 1\n",
        "\n",
        "        idx = 2\n",
        "        for word in wordfreq.keys():\n",
        "          vocab[word] =  idx\n",
        "          idx += 1\n",
        "\n",
        "        return vocab\n",
        "\n",
        "\n",
        "\n",
        "    # To tensor\n",
        "    def toTensor(self) -> torch.LongTensor:\n",
        "        max_len = self.max_len \n",
        "        lines = self.tokenize('sentence') \n",
        "        vocab = self.build_vocab()\n",
        "        encoded = []\n",
        "        for line in lines: \n",
        "          temp = []\n",
        "          for w in line: #각 줄에서 1개씩 글자를 읽음\n",
        "            try:\n",
        "              temp.append(vocab[w]) # 글자를 해당되는 정수로 변환\n",
        "            except KeyError: # 단어 집합에 없는 단어일 경우 unk로 대체된다.\n",
        "              temp.append(vocab['unk_idx']) # unk의 인덱스로 변환\n",
        "\n",
        "          if len(line) < max_len: # 현재 샘플이 정해준 길이보다 짧으면\n",
        "              temp += [vocab['padding_idx']] * (max_len - len(line)) # 나머지는 전부 'pad' 토큰으로 채운다.  \n",
        "          encoded.append(temp)\n",
        "        \n",
        "        tensor = torch.FloatTensor(encoded)\n",
        "        word_embedding = tensor.type(torch.LongTensor)\n",
        "        return word_embedding\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def __len__(self): \n",
        "        return len(self.data)\n",
        "\n",
        "\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        y = torch.tensor(self.data['isSpam']).unsqueeze(1)\n",
        "        word_embedding = self.toTensor()\n",
        "        y = y.type(torch.LongTensor)\n",
        "        item = {}\n",
        "        item['input'] = word_embedding\n",
        "        item['label'] = y\n",
        "        return item\n",
        "\n",
        "\n",
        "\n",
        " # your code"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaWs9hhsXOVL"
      },
      "source": [
        "dataset = MailDataset()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZiKwhufUAEe"
      },
      "source": [
        "seq_tensor =  dataset[0]['input']\n",
        "seq_lengths = torch.LongTensor(list(map(len, seq_tensor)))\n",
        "label = dataset[0]['label']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gs90V1Gv0t65"
      },
      "source": [
        "import torch.utils.data.sampler as splr\n",
        "\n",
        "class CustomDataLoader(object):\n",
        "  def __init__(self, seq_tensor, seq_lengths, label_tensor, batch_size):\n",
        "    self.batch_size = batch_size\n",
        "    self.seq_tensor = seq_tensor\n",
        "    self.seq_lengths = seq_lengths\n",
        "    self.label_tensor = label_tensor\n",
        "    self.sampler = splr.BatchSampler(splr.RandomSampler(self.label_tensor), self.batch_size, False)\n",
        "    self.sampler_iter = iter(self.sampler)\n",
        "    \n",
        "  def __iter__(self):\n",
        "    self.sampler_iter = iter(self.sampler) # reset sampler iterator\n",
        "    return self\n",
        "\n",
        "  def _next_index(self):\n",
        "    return next(self.sampler_iter) # may raise StopIteration\n",
        "\n",
        "  def __next__(self):\n",
        "    index = self._next_index()\n",
        "\n",
        "    subset_seq_tensor = self.seq_tensor[index]\n",
        "    subset_seq_lengths = self.seq_lengths[index]\n",
        "    subset_label_tensor = self.label_tensor[index]\n",
        "\n",
        "    subset_seq_lengths, perm_idx = subset_seq_lengths.sort(0, descending=True)\n",
        "    subset_seq_tensor = subset_seq_tensor[perm_idx]\n",
        "    subset_label_tensor = subset_label_tensor[perm_idx]\n",
        "\n",
        "    return subset_seq_tensor, subset_seq_lengths, subset_label_tensor\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.sampler)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFqy5XhG0-0J",
        "outputId": "fc77a1e3-61c2-459a-ef6d-51fddb2b46a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# shuffle data\n",
        "shuffled_idx = torch.randperm(label.shape[0])\n",
        "\n",
        "seq_tensor = seq_tensor[shuffled_idx]\n",
        "seq_lenghts = seq_lengths[shuffled_idx]\n",
        "label = label[shuffled_idx]\n",
        "\n",
        "# divide data into 3 sets\n",
        "PCT_TRAIN = 0.7 # 70% of data will be train set \n",
        "PCT_VALID = 0.2 # 20% of data will be validation set\n",
        "# The rest of data will be test set\n",
        "\n",
        "length = len(label)\n",
        "train_seq_tensor = seq_tensor[:int(length*PCT_TRAIN)] \n",
        "train_seq_lengths = seq_lengths[:int(length*PCT_TRAIN)]\n",
        "train_label = label[:int(length*PCT_TRAIN)]\n",
        "\n",
        "valid_seq_tensor = seq_tensor[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_seq_lengths = seq_lengths[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))] \n",
        "valid_label = label[int(length*PCT_TRAIN):int(length*(PCT_TRAIN+PCT_VALID))]\n",
        "\n",
        "test_seq_tensor = seq_tensor[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_seq_lengths = seq_lengths[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "test_label = label[int(length*(PCT_TRAIN+PCT_VALID)):]\n",
        "\n",
        "print(train_seq_tensor.shape) # torch.Size([4200, 30772])\n",
        "print(valid_seq_tensor.shape) # torch.Size([1199, 30772])\n",
        "print(test_seq_tensor.shape) # torch.Size([601, 30772])\n",
        "\n",
        "# Instantiate data loaders\n",
        "batch_size = 80\n",
        "train_loader = CustomDataLoader(train_seq_tensor, train_seq_lengths, train_label, batch_size)\n",
        "valid_loader = CustomDataLoader(valid_seq_tensor, valid_seq_lengths, valid_label, batch_size)\n",
        "test_loader = CustomDataLoader(test_seq_tensor, test_seq_lengths, test_label, batch_size)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3900, 80])\n",
            "torch.Size([1114, 80])\n",
            "torch.Size([558, 80])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-tLiPT4GU4o"
      },
      "source": [
        "### 훈련 인스턴스를 사용해서 train 함수를 통해 training을 해주시고,\n",
        "### eval 함수를 통해 40개의 test example에 대해서 accuracy를 측정해주세요.\n",
        "### 함수 및 클래스 signature와 내부 코드는 적절히 알아서 짜주시면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk9d0rXxGU4o"
      },
      "source": [
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class Seq2SeqAttention(nn.Module):\n",
        "    def __init__(self, config, vocab_size, word_embeddings):\n",
        "        super(Seq2SeqAttention, self).__init__()\n",
        "        self.config = config\n",
        "        \n",
        "        # Embedding Layer\n",
        "        self.embeddings = nn.Embedding(vocab_size, self.config.embed_size)\n",
        "        self.embeddings.weight = nn.Parameter(word_embeddings, requires_grad=False)\n",
        "        \n",
        "        # Encoder RNN\n",
        "        self.lstm = nn.LSTM(input_size = self.config.embed_size,\n",
        "                            hidden_size = self.config.hidden_size,\n",
        "                            num_layers = self.config.hidden_layers,\n",
        "                            bidirectional = self.config.bidirectional)\n",
        "        \n",
        "        # Dropout Layer\n",
        "        self.dropout = nn.Dropout(self.config.dropout_keep)\n",
        "        \n",
        "        # Fully-Connected Layer\n",
        "        self.fc = nn.Linear(\n",
        "            self.config.hidden_size * (1+self.config.bidirectional) * 2,\n",
        "            self.config.output_size\n",
        "        )\n",
        "        \n",
        "        # Softmax non-linearity\n",
        "        self.softmax = nn.Softmax()\n",
        "                \n",
        "    def apply_attention(self, rnn_output, final_hidden_state):\n",
        "        '''\n",
        "        Apply Attention on RNN output\n",
        "        \n",
        "        Input:\n",
        "            rnn_output (batch_size, seq_len, num_directions * hidden_size): tensor representing hidden state for every word in the sentence\n",
        "            final_hidden_state (batch_size, num_directions * hidden_size): final hidden state of the RNN\n",
        "            \n",
        "        Returns:\n",
        "            attention_output(batch_size, num_directions * hidden_size): attention output vector for the batch\n",
        "        '''\n",
        "        hidden_state = final_hidden_state.unsqueeze(2)\n",
        "        attention_scores = torch.bmm(rnn_output, hidden_state).squeeze(2)\n",
        "        soft_attention_weights = F.softmax(attention_scores, 1).unsqueeze(2) #shape = (batch_size, seq_len, 1)\n",
        "        attention_output = torch.bmm(rnn_output.permute(0,2,1), soft_attention_weights).squeeze(2)\n",
        "        return attention_output\n",
        "        \n",
        "    def forward(self, x):\n",
        "        max_sen_len, = self.config.max_sen_len\n",
        "        x.shape = (max_sen_len, batch_size)\n",
        "        embedded_sent = self.embeddings(x)\n",
        "        embedded_sent.shape = (max_sen_len, batch_size,embed_size)\n",
        "\n",
        "        ##################################### Encoder #######################################\n",
        "        lstm_output, (h_n,c_n) = self.lstm(embedded_sent)\n",
        "        lstm_output.shape = (seq_len, batch_size, num_directions * hidden_size)\n",
        "        \n",
        "        #Final hidden state of last layer (num_directions, batch_size, hidden_size)\n",
        "        batch_size = h_n.shape[1]\n",
        "        h_n_final_layer = h_n.view(self.config.hidden_layers,\n",
        "                                   self.config.bidirectional + 1,\n",
        "                                   batch_size,\n",
        "                                   self.config.hidden_size)[-1,:,:,:]\n",
        "        \n",
        "        ##################################### Attention #####################################\n",
        "        # Convert input to (batch_size, num_directions * hidden_size) for attention\n",
        "        final_hidden_state = torch.cat([h_n_final_layer[i,:,:] for i in range(h_n_final_layer.shape[0])], dim=1)\n",
        "        \n",
        "        attention_out = self.apply_attention(lstm_output.permute(1,0,2), final_hidden_state)\n",
        "        Attention_out.shape = (batch_size, num_directions * hidden_size)\n",
        "        \n",
        "        #################################### Linear #########################################\n",
        "        concatenated_vector = torch.cat([final_hidden_state, attention_out], dim=1)\n",
        "        final_feature_map = self.dropout(concatenated_vector) # shape=(batch_size, num_directions * hidden_size)\n",
        "        final_out = self.fc(final_feature_map)\n",
        "        return self.softmax(final_out)\n",
        "    \n",
        "    def add_optimizer(self, optimizer):\n",
        "        self.optimizer = optimizer\n",
        "        \n",
        "    def add_loss_op(self, loss_op):\n",
        "        self.loss_op = loss_op\n",
        "    \n",
        "    def reduce_lr(self):\n",
        "        print(\"Reducing LR\")\n",
        "        for g in self.optimizer.param_groups:\n",
        "            g['lr'] = g['lr'] / 2\n",
        "                \n",
        "    def run_epoch(self, train_iterator, val_iterator, epoch):\n",
        "        train_losses = []\n",
        "        val_accuracies = []\n",
        "        losses = []\n",
        "        \n",
        "        # Reduce learning rate as number of epochs increase\n",
        "        if (epoch == int(self.config.max_epochs/3)) or (epoch == int(2*self.config.max_epochs/3)):\n",
        "            self.reduce_lr()\n",
        "            \n",
        "        for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "            self.optimizer.zero_grad()\n",
        "            if torch.cuda.is_available():\n",
        "                x = seq_tensor\n",
        "                y = (label - 1).type(torch.cuda.LongTensor)\n",
        "            else:\n",
        "                x = seq_tensor\n",
        "                y = (label - 1).type(torch.LongTensor)\n",
        "            y_pred = self.__call__(x)\n",
        "            loss = self.loss_op(y_pred, y)\n",
        "            loss.backward()\n",
        "            losses.append(loss.data.cpu().numpy())\n",
        "            self.optimizer.step()\n",
        "    \n",
        "            if i % 100 == 0:\n",
        "                print(\"Iter: {}\".format(i+1))\n",
        "                avg_train_loss = np.mean(losses)\n",
        "                train_losses.append(avg_train_loss)\n",
        "                print(\"\\tAverage training loss: {:.5f}\".format(avg_train_loss))\n",
        "                losses = []\n",
        "                \n",
        "                # Evalute Accuracy on validation set\n",
        "                val_accuracy = evaluate_model(self, val_iterator)\n",
        "                print(\"\\tVal Accuracy: {:.4f}\".format(val_accuracy))\n",
        "                self.train()\n",
        "                \n",
        "        return train_losses, val_accuracies\n"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33h4Z-M3UUmG"
      },
      "source": [
        "class Config(object):\n",
        "    vocab_size = len(seq_tensor_lengths)\n",
        "    embed_size = int(vocab_size ** 0.25)\n",
        "    hidden_layers = 1\n",
        "    hidden_size = 32\n",
        "    bidirectional = True\n",
        "    output_size = 4\n",
        "    max_epochs = 15\n",
        "    lr = 0.5\n",
        "    batch_size = 80\n",
        "    dropout_keep = 0.8\n",
        "    max_sen_len = seq_tensor_lengths[0] # Sequence length for RNN"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-da31xSxh2XD"
      },
      "source": [
        "config = Config()"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbuugbqRZ--3",
        "outputId": "4859a929-e5a1-47e0-c6dc-6ffca70f997a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 404
        }
      },
      "source": [
        "device = torch.device(\"cuda\") \n",
        "\n",
        "\n",
        "\n",
        "if __name__=='__main__':\n",
        "   \n",
        "    for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "\n",
        "        seq_tensor = seq_tensor.to(device)\n",
        "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "    for seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "        valid_tensor = seq_tensor.to(device)\n",
        "        valid_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "        valid_label = label.to(device)\n",
        "\n",
        "\n",
        "    \n",
        "    # Create Model with specified optimizer and loss function\n",
        "    ##############################################################\n",
        "    model = Seq2SeqAttention(config, int(seq_tensor_lengths[0]), seq_tensor)\n",
        "    if torch.cuda.is_available():\n",
        "        model.cuda()\n",
        "    model.train()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=config.lr)\n",
        "    NLLLoss = nn.NLLLoss()\n",
        "    model.add_optimizer(optimizer)\n",
        "    model.add_loss_op(NLLLoss)\n",
        "    ##############################################################\n",
        "    \n",
        "    train_losses = []\n",
        "    val_accuracies = []\n",
        "    \n",
        "    for i in range(config.max_epochs):\n",
        "        print (\"Epoch: {}\".format(i))\n",
        "        train_loss,val_accuracy = model.run_epoch(train_loader, valid_loader, i)\n",
        "        train_losses.append(train_loss)\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "    train_acc = evaluate_model(model, dataset.train_iterator)\n",
        "    val_acc = evaluate_model(model, dataset.val_iterator)\n",
        "    test_acc = evaluate_model(model, dataset.test_iterator)\n",
        "\n",
        "    print ('Final Training Accuracy: {:.4f}'.format(train_acc))\n",
        "    print ('Final Validation Accuracy: {:.4f}'.format(val_acc))\n",
        "    print ('Final Test Accuracy: {:.4f}'.format(test_acc))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-100-5d0fbba80ff9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mval_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-7d2f02e48888>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(self, train_iterator, val_iterator, epoch)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-7d2f02e48888>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mmax_sen_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_sen_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_sen_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0membedded_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 450\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iteration over a 0-d tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    451\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             warnings.warn('Iterating over a tensor might cause the trace to be incorrect. '\n",
            "\u001b[0;31mTypeError\u001b[0m: iteration over a 0-d tensor"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMta1KnkjmmX"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "class SpamHamLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, n_layers,\n",
        "                 drop_lstm=0.1, drop_out = 0.1):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        # embedding \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # LSTM layers\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
        "                            dropout=drop_lstm, batch_first=True)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(drop_out)\n",
        "        \n",
        "        # linear and sigmoid layers\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "\n",
        "    def forward(self, x, seq_lengths):\n",
        "\n",
        "        # embeddings\n",
        "        embedded_seq_tensor = self.embedding(x)\n",
        "                \n",
        "        # pack, remove pads\n",
        "        packed_input = pack_padded_sequence(embedded_seq_tensor, seq_lengths.cpu().numpy(), batch_first=True)\n",
        "        \n",
        "        # lstm\n",
        "        packed_output, (ht, ct) = self.lstm(packed_input, None)\n",
        "          # https://pytorch.org/docs/stable/_modules/torch/nn/modules/rnn.html\n",
        "          # If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero\n",
        "\n",
        "        # unpack, recover padded sequence\n",
        "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
        "       \n",
        "        # collect the last output in each batch\n",
        "        last_idxs = (input_sizes - 1).to(device) # last_idxs = input_sizes - torch.ones_like(input_sizes)\n",
        "        output = torch.gather(output, 1, last_idxs.view(-1, 1).unsqueeze(2).repeat(1, 1, self.hidden_dim)).squeeze() # [batch_size, hidden_dim]\n",
        "        \n",
        "        # dropout and fully-connected layer\n",
        "        output = self.dropout(output)\n",
        "        output = self.fc(output).squeeze()\n",
        "               \n",
        "        # sigmoid function\n",
        "        output = self.sig(output)\n",
        "        \n",
        "        return output"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dzZG6Acp1HM",
        "outputId": "ffce4b69-5526-4723-cd91-05c4a1e7742c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# training params\n",
        "\n",
        "epochs = 6 \n",
        "\n",
        "counter = 0\n",
        "print_every = 10\n",
        "clip=5 # gradient clipping\n",
        "\n",
        "vocab_size = len(dataset.build_vocab())\n",
        "embedding_dim = 80\n",
        "hidden_dim = 15\n",
        "output_size = 1\n",
        "n_layers = 4\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "net= SpamHamLSTM(vocab_size,embedding_dim , hidden_dim,output_size, n_layers)\n",
        "net.train()\n",
        "# train for some number of epochs\n",
        "val_losses = []\n",
        "for e in range(epochs):\n",
        "\n",
        "\n",
        "    for seq_tensor, seq_tensor_lengths, label in iter(train_loader):\n",
        "        counter += 1\n",
        "               \n",
        "        seq_tensor = seq_tensor.to(device)\n",
        "        seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "        label = label.to(device)\n",
        " \n",
        "        # get the output from the model\n",
        "        output = net(seq_tensor, seq_tensor_lengths)\n",
        "    \n",
        "        # get the loss and backprop\n",
        "        loss = criterion(output, label.float())\n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()\n",
        "        \n",
        "        # prevent the exploding gradient\n",
        "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        # loss stats\n",
        "        if counter % print_every == 0:\n",
        "            # Get validation loss\n",
        "            \n",
        "            val_losses_in_itr = []\n",
        "            sums = []\n",
        "            sizes = []\n",
        "            \n",
        "            net.eval()\n",
        "            \n",
        "            for seq_tensor, seq_tensor_lengths, label in iter(valid_loader):\n",
        "\n",
        "                seq_tensor = seq_tensor.to(device)\n",
        "                seq_tensor_lengths = seq_tensor_lengths.to(device)\n",
        "                label = label.to(device)\n",
        "                output = net(seq_tensor, seq_tensor_lengths)\n",
        "                \n",
        "                # losses\n",
        "                val_loss = criterion(output, label.float())     \n",
        "                val_losses_in_itr.append(val_loss.item())\n",
        "                \n",
        "                # accuracy\n",
        "                binary_output = (output >= 0.5).short() # short(): torch.int16\n",
        "                right_or_not = torch.eq(binary_output, label)\n",
        "                sums.append(torch.sum(right_or_not).float().item())\n",
        "                sizes.append(right_or_not.shape[0])\n",
        "            \n",
        "            accuracy = sum(sums) / sum(sizes)\n",
        "            \n",
        "            net.train()\n",
        "            print(\"Epoch: {:2d}/{:2d}\\t\".format(e+1, epochs),\n",
        "                  \"Steps: {:3d}\\t\".format(counter),\n",
        "                  \"Loss: {:.6f}\\t\".format(loss.item()),\n",
        "                  \"Val Loss: {:.6f}\\t\".format(np.mean(val_losses_in_itr)),\n",
        "                  \"Accuracy: {:.3f}\".format(accuracy))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-120-9f743dad13ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m# get the output from the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_tensor_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# get the loss and backprop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-105-f0b9dcaad641>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seq_lengths)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0membedded_seq_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# pack, remove pads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_index_select"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUHD3LMMqlYg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}